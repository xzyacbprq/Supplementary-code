{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading datasets:\n",
    "\n",
    "1. download image sequences from: http://www.cvlibs.net/download.php?file=data_odometry_color.zip\n",
    "2. download ground truth labels from: http://www.cvlibs.net/download.php?file=data_odometry_poses.zip\n",
    "3. download flow ground truth from: https://drive.google.com/file/d/1U9FajKofxIIpT4lRR0oAYgV1plxzRo4f/view?usp=sharing\n",
    "3. Unzip the downloaded contents in the 'path_to_dataset' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= True\n",
    "debug=False\n",
    "epochs=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name='exp_name/'\n",
    "data_root='path_to_data_dirr\\\\'\n",
    "rep_root='path_to_working_dirr\\\\'\n",
    "notebook_name='prf_training_notebook_original.ipynb'\n",
    "load_random_vars=False\n",
    "load_init_wts=False\n",
    "random_vars_dir=rep_root+'PRF\\\\logs\\\\load_vars\\\\'\n",
    "log_dir=rep_root+'PRF\\\\logs\\\\'\n",
    "start_epoch=1\n",
    "#wt_path=rep_root+'Flow-Net\\\\logs\\\\exp9-lr-0.0001-ns-1.0\\\\exp9-lr-0.0001-ns-1.0best_wts.hdf5'\n",
    "wt_path=''#rep_root+'Flow-Net\\\\logs\\\\exp6-lr-0.0001-ns-1.0\\\\exp6-lr-0.0001-ns-1.0best_wts.hdf5'\n",
    "\n",
    "\n",
    "pretrain_exp_name=None#'exp-M49\\\\'#-layer_not_trained_0-lr-0.0005-odom_wt/'\n",
    "start_epoch+=0\n",
    "\n",
    "if pretrain_exp_name == None :\n",
    "    pre_trained_wts_path=None#log_dir+pretrain_exp_name+str(start_epoch-1)+\".hdf5\"#log_dir+exp_name+\"10.hdf5\"\n",
    "    pre_trained_opt_path=None#log_dir+pretrain_exp_name+str(start_epoch-1)+\".pkl\"\n",
    "    pre_trained_log_path=None#log_dir+pretrain_exp_name+'loss_epoch.csv'\n",
    "else :\n",
    "    pre_trained_wts_path=log_dir+pretrain_exp_name+str(start_epoch-1)+\".hdf5\"\n",
    "    pre_trained_opt_path=log_dir+pretrain_exp_name+str(start_epoch-1)+\".pkl\"\n",
    "    pre_trained_log_path=log_dir+pretrain_exp_name+'loss_epoch.csv'\n",
    "\n",
    "#best_wts_path=log_dir+exp_name+'best_model'+'.h5'\n",
    "best_wts_path=log_dir+exp_name+'best_model_rot'+'.h5'\n",
    "#best_wts_path=log_dir+exp_name+'best_model_trans'+'.h5'\n",
    "\n",
    "img_root=data_root+'data_odometry_color\\\\dataset\\\\sequences\\\\'\n",
    "label_root=data_root+'data_odometry_poses\\\\dataset\\\\poses\\\\'\n",
    "seed=1\n",
    "drop_seed=seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from __future__ import print_function\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.random.set_seed(seed)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense ,Input,concatenate ,Conv2D,Conv2DTranspose,\\\n",
    "MaxPooling2D,AveragePooling2D, LSTM ,Reshape, TimeDistributed,ReLU, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "from collections import deque\n",
    "from copy import copy,deepcopy\n",
    "from tqdm import tqdm,trange\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "image_w= 320# 1280\n",
    "image_h= 128#384\n",
    "time_step=2\n",
    "overlap=1\n",
    "if train ==False:\n",
    "    overlap=time_step-1\n",
    "start_points=[0]#,2,4]\n",
    "no_of_pose=6\n",
    "\n",
    "#all_files=[0,1,2,3,4,5,6,7,8,9,10]\n",
    "train_files=[0,1,2,3,6,8,9]\n",
    "val_files = [5,7,10]#7,10]\n",
    "test_files= [4,5,7,10]\n",
    "\n",
    "#learning rate scheduling parameters\n",
    "batch_size=16\n",
    "learning_rate=0.0005\n",
    "_lr_decay_enable=0\n",
    "_lr_decay_ratio = 2\n",
    "_lr_decay_epoch=200\n",
    "_lr_min=0.0001\n",
    "_grad_clip=0.000002#1e-6\n",
    "\n",
    "scale=2\n",
    "#label_scale='std_scl'\n",
    "label_scale='diff_div'\n",
    "#label_scale='diff_scl'\n",
    "_img_norm=False\n",
    "_img_offset=0.5\n",
    "_keep_yaw_only=False\n",
    "_flow_scale=2\n",
    "_flow_format='hsv'\n",
    "_flow_loss='rmsle'\n",
    "_flow_chanel=2\n",
    "_flow_decompress=False\n",
    "\n",
    "_disp_or_seg='disp'\n",
    "_pose_loss='mae'\n",
    "_disp_scale=2\n",
    "_disp_loss=_flow_loss\n",
    "_disp_chanel=1\n",
    "\n",
    "time_dis=False\n",
    "repeat_data=False\n",
    "\n",
    "_grad_lr=0.00000 \n",
    "_grad_alpha=1.5\n",
    "_init_loss_wts=np.array([1,10,0.1,0],dtype=np.float32)\n",
    "_mtl_main_task_index=1\n",
    "_mtl_aux_grad_algo='v4'\n",
    "\n",
    "_mtl_cosine_use_fact=0.0\n",
    "_mtl_cos_alpha=1.0\n",
    "_mtl_cos_center=1.0\n",
    "_mtl_cosine_avg='eqn_1' \n",
    "\n",
    "_mtl_use_loss_slope='none'\n",
    "_mtl_loss_center=1.0\n",
    "_mtl_loss_slope_center=1.0\n",
    "_mtl_loss_slope_alpha=1.0\n",
    "\n",
    "\n",
    "\n",
    "_mtl_grad_sign_fact=0.1\n",
    "_mtl_grad_diff='square'\n",
    "\n",
    "_mtl_get_variance_from='calc'\n",
    "_mtl_grad_var_calc_every=1\n",
    "_mtl_update_grad_var=1\n",
    "_mtl_grad_var_alpha=1.0\n",
    "_mtl_div_with_rolling_mean=False\n",
    "_mtl_use_std_ratio=True\n",
    "_mtl_use_factors='default' #'ratio_only' or 'coef_only' or 'default'\n",
    "_mtl_beta_norm_method='none'\n",
    "_mtl_beta_use_fact=1.0\n",
    "_mtl_take_log_prob=False\n",
    "_mtl_use_cosine_v4=False\n",
    "_mtl_use_mode_v4='mode-1'\n",
    "_mtl_mode_2_alpha_v4=0.5\n",
    "\n",
    "#loss weights decay parameters\n",
    "_disp_weight_decay_enable=0\n",
    "_disp_weight_decay_ratio = 100\n",
    "_disp_weight_decay_epoch=10\n",
    "_disp_weight_min=0.0001\n",
    "\n",
    "_flow_weight_decay_enable=0\n",
    "_flow_weight_decay_ratio = 10\n",
    "_flow_weight_decay_epoch=10\n",
    "_flow_weight_min=0.0001\n",
    "\n",
    "#curr_optimizer=Adagrad(lr=learning_rate)#,clipvalue=_grad_clip)\n",
    "#opt=Adam(lr=learning_rate)#, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "curr_optimizer=Adam(lr=learning_rate)\n",
    "#grad_optimizer=Adam(lr=_grad_lr)\n",
    "class Net_Params():\n",
    "    def __init__(self,_scale):\n",
    "        self.N=64//_scale\n",
    "        N=self.N\n",
    "        self._num_shared_layer=9\n",
    "        \n",
    "        self._kernels        =[7, 5,  5,  3,  3,  3,  3,  3,   3,    3]\n",
    "        self._filters        =[N,2*N,4*N,4*N,8*N,8*N,8*N,8*N,16*N,16*N]\n",
    "        self._strides        =[2, 2,  2,  1,  2,  1,  2,  1,   2,   1]\n",
    "        self._block          =[1, 2,  3,  3,  4,  4,  5,  5,   6,   6]\n",
    "        self._nconv          =[1, 1,  1,  2,  1,  2,  1,  2,   1,   2]\n",
    "        self._act            =[1, 1,  1,  1,  1,  1,  1,  1,   1,   1]\n",
    "        self._batch_norm     =[1, 1,  1,  1,  1,  1,  1,  1,   1,   1]\n",
    "        self._drop_out       =[0, 0,  0,  0,  0,  0,  0,  0,   0,   0] \n",
    "        self._act[self._num_shared_layer-1]=0\n",
    "        self._drop_out[self._num_shared_layer-1]=0.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self._num_trans_layers=2\n",
    "        self._trans_layer_nodes  = [512,512,512]\n",
    "        self._trans_act=[1,1,0]\n",
    "        self._trans_drop_out=[0,0,0]\n",
    "        \n",
    "        self._num_rot_layers=2\n",
    "        self._rot_layer_nodes = [512,512,512]\n",
    "        self._rot_act=[1,1,0]\n",
    "        self._rot_drop_out=[0,0,0]\n",
    "        \n",
    "        self.drop_seed       =seed\n",
    "        self._leaky_relu     = True\n",
    "        self._relu           = False\n",
    "        self._kernel_initializer='he_normal'\n",
    "        self._kernel_regularizer=None\n",
    "        \n",
    "        self._deconv_filters=[8*N,4*N,2*N,1*N]\n",
    "        self._deconv_act =[1,1,1,1]\n",
    "        self._flow_chanel =_flow_chanel\n",
    "        self._disp_chanel =_disp_chanel\n",
    "        \n",
    "        \n",
    "        \n",
    "_net_par=Net_Params(scale)\n",
    "\n",
    "grad_clip_layers= {\n",
    "    'block_1_conv_1/kernel:0':_grad_clip,\n",
    "    'block_2_conv_1/kernel:0':_grad_clip,\n",
    "    'block_3_conv_1/kernel:0':_grad_clip,\n",
    "    'block_3_conv_2/kernel:0':_grad_clip,\n",
    "    'block_4_conv_1/kernel:0':_grad_clip,\n",
    "    'block_4_conv_2/kernel:0':_grad_clip,\n",
    "    'block_5_conv_1/kernel:0':_grad_clip,\n",
    "    'block_5_conv_2/kernel:0':_grad_clip,\n",
    "    'block_6_conv_1/kernel:0':_grad_clip,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 32,\n",
       " '_num_shared_layer': 9,\n",
       " '_kernels': [7, 5, 5, 3, 3, 3, 3, 3, 3, 3],\n",
       " '_filters': [32, 64, 128, 128, 256, 256, 256, 256, 512, 512],\n",
       " '_strides': [2, 2, 2, 1, 2, 1, 2, 1, 2, 1],\n",
       " '_block': [1, 2, 3, 3, 4, 4, 5, 5, 6, 6],\n",
       " '_nconv': [1, 1, 1, 2, 1, 2, 1, 2, 1, 2],\n",
       " '_act': [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       " '_batch_norm': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " '_drop_out': [0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0],\n",
       " '_num_trans_layers': 2,\n",
       " '_trans_layer_nodes': [512, 512, 512],\n",
       " '_trans_act': [1, 1, 0],\n",
       " '_trans_drop_out': [0, 0, 0],\n",
       " '_num_rot_layers': 2,\n",
       " '_rot_layer_nodes': [512, 512, 512],\n",
       " '_rot_act': [1, 1, 0],\n",
       " '_rot_drop_out': [0, 0, 0],\n",
       " 'drop_seed': 1,\n",
       " '_leaky_relu': True,\n",
       " '_relu': False,\n",
       " '_kernel_initializer': 'he_normal',\n",
       " '_kernel_regularizer': None,\n",
       " '_deconv_filters': [256, 128, 64, 32],\n",
       " '_deconv_act': [1, 1, 1, 1],\n",
       " '_flow_chanel': 2,\n",
       " '_disp_chanel': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_net_par.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_str=''\n",
    "param_str+='exp_name : '+str(exp_name)+'\\n'\n",
    "param_str+='notebook_name : '+str(notebook_name)+'\\n'\n",
    "param_str+='load_random_vars : '+str(load_random_vars)+'\\n'\n",
    "param_str+='load_init_wts : '+str(load_init_wts)+'\\n'\n",
    "param_str+='height : '+str(image_h)+'\\n'\n",
    "param_str+='width : '+str(image_w)+'\\n'\n",
    "param_str+='time_step : '+str(time_step)+'\\n'\n",
    "param_str+='overlap : '+str(overlap)+'\\n'\n",
    "param_str+='start_point : '+str(start_points)+'\\n'\n",
    "param_str+='img_offset : '+str(_img_offset)+'\\n'\n",
    "param_str+='img_norm : '+str(_img_norm)+'\\n'\n",
    "param_str+='keep_yaw_only : '+str(_keep_yaw_only)+'\\n'\n",
    "param_str+='label scale : '+str(label_scale)+'\\n'\n",
    "param_str+='repeat_data : '+str(repeat_data)+'\\n'\n",
    "param_str+='pose loss : '+str(_pose_loss)+'\\n'\n",
    "param_str+='learning_rate : '+str(learning_rate)+'\\n'\n",
    "param_str+='lr_decay_enable : '+str(_lr_decay_enable)+'\\n'\n",
    "param_str+='lr_decay_ratio : '+str(_lr_decay_ratio)+'\\n'\n",
    "param_str+='lr_decay_epoch : '+str(_lr_decay_epoch)+'\\n'\n",
    "param_str+='lr_min : '+str(_lr_min)+'\\n'\n",
    "param_str+='grad_lr : '+str(_grad_lr)+'\\n'\n",
    "param_str+='grad_alpha : '+str(_grad_alpha)+'\\n'\n",
    "param_str+='grad_clip : '+str(_grad_clip)+'\\n'\n",
    "param_str+='epochs : '+str(epochs)+'\\n'\n",
    "param_str+='batch_size : '+str(batch_size)+'\\n'\n",
    "param_str+='net_scale : '+str(scale)+'\\n'\n",
    "param_str+='init_loss_wts : '+str(_init_loss_wts)+'\\n'\n",
    "\n",
    "param_str+='disp_or_seg : '+str(_disp_or_seg)+'\\n'\n",
    "param_str+='disp_weight_decay_enable : '+str(_disp_weight_decay_enable)+'\\n'\n",
    "param_str+='disp_weight_decay_ratio : '+str(_disp_weight_decay_ratio)+'\\n'\n",
    "param_str+='disp_weight_decay_epoch : '+str(_disp_weight_decay_epoch)+'\\n'\n",
    "param_str+='disp_weight_min : '+str(_disp_weight_min)+'\\n'\n",
    "\n",
    "param_str+='flow_weight_decay_enable : '+str(_flow_weight_decay_enable)+'\\n'\n",
    "param_str+='flow_weight_decay_ratio : '+str(_flow_weight_decay_ratio)+'\\n'\n",
    "param_str+='flow_weight_decay_epoch : '+str(_flow_weight_decay_epoch)+'\\n'\n",
    "param_str+='flow_weight_min : '+str(_flow_weight_min)+'\\n'\n",
    "\n",
    "param_str+='of_scale : '+str(_flow_scale)+'\\n'\n",
    "param_str+='flow_format : '+str(_flow_format)+'\\n'\n",
    "param_str+='flow_loss : '+str(_flow_loss)+'\\n'\n",
    "param_str+='disp_scale : '+str(_disp_scale)+'\\n'\n",
    "param_str+='disp_loss : '+str(_disp_loss)+'\\n'\n",
    "\n",
    "param_str+='mtl_main_task_index : '+str(_mtl_main_task_index)+'\\n'\n",
    "param_str+='mtl_aux_grad_algo : '+str(_mtl_aux_grad_algo)+'\\n'\n",
    "\n",
    "param_str+='mtl_cosine_use_fact : ' +str(_mtl_cosine_use_fact)+'\\n'\n",
    "param_str+='mtl_cos_alpha : ' +str(_mtl_cos_alpha)+'\\n'\n",
    "param_str+='mtl_cos_center : ' +str(_mtl_cos_center)+'\\n'\n",
    "param_str+='mtl_cosine_avg : ' +str(_mtl_cosine_avg)+'\\n'\n",
    "\n",
    "param_str+='mtl_use_loss_slope : ' +str(_mtl_use_loss_slope)+'\\n'\n",
    "param_str+='mtl_loss_slope_center : '+str(_mtl_loss_slope_center)+'\\n'\n",
    "param_str+='mtl_loss_high_offset : '+str(_mtl_loss_center)+'\\n'\n",
    "param_str+='mtl_loss_slope_alpha : '+str(_mtl_loss_slope_alpha)+'\\n'\n",
    "\n",
    "param_str+='mtl_grad_sign_fact : ' +str(_mtl_grad_sign_fact)+'\\n'\n",
    "param_str+='mtl_grad_diff : ' +str(_mtl_grad_diff)+'\\n'\n",
    "\n",
    "param_str+='mtl_get_variance_from : ' +str(_mtl_get_variance_from)+'\\n'\n",
    "param_str+='mtl_grad_var_calc_every : ' +str(_mtl_grad_var_calc_every)+'\\n'\n",
    "param_str+='mtl_update_grad_var : ' +str(_mtl_update_grad_var)+'\\n'\n",
    "param_str+='mtl_grad_var_alpha : ' +str(_mtl_grad_var_alpha)+'\\n'\n",
    "param_str+='mtl_div_with_rolling_mean : ' +str(_mtl_div_with_rolling_mean)+'\\n'\n",
    "\n",
    "param_str+='mtl_use_std_ratio : ' +str(_mtl_use_std_ratio)+'\\n'\n",
    "param_str+='mtl_use_factors : ' +str(_mtl_use_factors)+'\\n'\n",
    "param_str+='mtl_beta_norm_method : ' +str(_mtl_beta_norm_method)+'\\n'\n",
    "param_str+='mtl_beta_use_fact : ' +str(_mtl_beta_use_fact)+'\\n'\n",
    "param_str+='mtl_prob_scale_fact : ' +str(_mtl_take_log_prob)+'\\n'\n",
    "\n",
    "\n",
    "param_str+='mtl_use_cosine_v4 : ' +str(_mtl_use_cosine_v4)+'\\n'\n",
    "param_str+='mtl_use_mode_v4 : ' +str(_mtl_use_mode_v4)+'\\n'\n",
    "param_str+='mtl_mode_2_alpha_v4 : ' +str(_mtl_mode_2_alpha_v4)+'\\n'\n",
    "\n",
    "for kk,val in _net_par.__dict__.items():\n",
    "    param_str+=str(kk)+' : '\n",
    "    param_str+=str(val)+'\\n'\n",
    "for kk,val in grad_clip_layers.items():\n",
    "    param_str+=str('clip_value - ')+str(kk)+' : '\n",
    "    param_str+=str(val)+'\\n'\n",
    "#param_str+='loss weights : '+str(loss_weights)+'\\n'\n",
    "param_str+='flow_wt_path : '+str(wt_path)+'\\n'\n",
    "param_str+='seed : '+str(seed)+'\\n'\n",
    "if train==True:\n",
    "    if not  os.path.exists(log_dir+exp_name): os.mkdir(log_dir+exp_name)\n",
    "    f=open(log_dir+exp_name+'params.txt','w+')\n",
    "    f.write(param_str)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_grad_log_epoch=1\n",
    "_calc_grad_log_epoch=1\n",
    "_all_wts_log_epoch=1\n",
    "_save_opts=False\n",
    "_grad_data_len=100#batch\n",
    "_train_flow=True\n",
    "_train_disp=False\n",
    "\n",
    "grad_log_layers= [\n",
    "    'block_1_conv_1/kernel:0',\n",
    "    'block_1_conv_1/bias:0',\n",
    "    'block_2_conv_1/kernel:0',\n",
    "    'block_2_conv_1/bias:0',\n",
    "    'block_3_conv_1/kernel:0',\n",
    "    'block_3_conv_1/bias:0',\n",
    "    'block_3_conv_2/kernel:0',\n",
    "    'block_3_conv_2/bias:0',\n",
    "    'block_4_conv_1/kernel:0',\n",
    "    'block_4_conv_1/bias:0',\n",
    "    'block_4_conv_2/kernel:0',\n",
    "    'block_4_conv_2/bias:0',\n",
    "    'block_5_conv_1/kernel:0',\n",
    "    'block_5_conv_1/bias:0',\n",
    "    'block_5_conv_2/kernel:0',\n",
    "    'block_5_conv_2/bias:0',\n",
    "    'block_6_conv_1/kernel:0',\n",
    "    'block_6_conv_1/bias:0',\n",
    "    'dense_1.1/kernel:0',\n",
    "    'dense_1.1/bias:0',\n",
    "    'dense_1.2/kernel:0',\n",
    "    'dense_1.2/bias:0',\n",
    "    'dense_2.1/kernel:0',\n",
    "    'dense_2.1/bias:0',\n",
    "    'dense_2.2/kernel:0',\n",
    "    'dense_2.2/bias:0'\n",
    "    ]\n",
    "    \n",
    "grad_norm_layers= [\n",
    "    'block_6_conv_1/kernel:0',\n",
    "    ]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving copy of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System time :  1607006544.2924528\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    }
   ],
   "source": [
    "if train==True:\n",
    "    curr_time=time.time()\n",
    "    print('System time : ',curr_time)\n",
    "    %autosave 1\n",
    "    time.sleep(3)\n",
    "    with open(notebook_name,'r') as file:\n",
    "        notebook_data=file.read()\n",
    "    with open(log_dir+exp_name+notebook_name+'_log_'+str(curr_time)+'.ipynb','w') as file:\n",
    "        file.write(notebook_data)\n",
    "    %autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler angle to transformation matrix conversion and vice varsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPS = np.finfo(float).eps * 4.0\n",
    "\n",
    "# axis sequences for Euler angles\n",
    "_NEXT_AXIS = [1, 2, 0, 1]\n",
    "\n",
    "# map axes strings to/from tuples of inner axis, parity, repetition, frame\n",
    "_AXES2TUPLE = {\n",
    "    'sxyz': (0, 0, 0, 0), 'sxyx': (0, 0, 1, 0), 'sxzy': (0, 1, 0, 0),\n",
    "    'sxzx': (0, 1, 1, 0), 'syzx': (1, 0, 0, 0), 'syzy': (1, 0, 1, 0),\n",
    "    'syxz': (1, 1, 0, 0), 'syxy': (1, 1, 1, 0), 'szxy': (2, 0, 0, 0),\n",
    "    'szxz': (2, 0, 1, 0), 'szyx': (2, 1, 0, 0), 'szyz': (2, 1, 1, 0),\n",
    "    'rzyx': (0, 0, 0, 1), 'rxyx': (0, 0, 1, 1), 'ryzx': (0, 1, 0, 1),\n",
    "    'rxzx': (0, 1, 1, 1), 'rxzy': (1, 0, 0, 1), 'ryzy': (1, 0, 1, 1),\n",
    "    'rzxy': (1, 1, 0, 1), 'ryxy': (1, 1, 1, 1), 'ryxz': (2, 0, 0, 1),\n",
    "    'rzxz': (2, 0, 1, 1), 'rxyz': (2, 1, 0, 1), 'rzyz': (2, 1, 1, 1)}\n",
    "\n",
    "_TUPLE2AXES = dict((v, k) for k, v in _AXES2TUPLE.items())\n",
    "\n",
    "def euler_to_matrix(ai, aj, ak, axes='syxz'):\n",
    "    try:\n",
    "        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes]\n",
    "    except (AttributeError, KeyError):\n",
    "        _TUPLE2AXES[axes]  # validation\n",
    "        firstaxis, parity, repetition, frame = axes\n",
    "\n",
    "    i = firstaxis\n",
    "    j = _NEXT_AXIS[i+parity]\n",
    "    k = _NEXT_AXIS[i-parity+1]\n",
    "\n",
    "    if frame:\n",
    "        ai, ak = ak, ai\n",
    "    if parity:\n",
    "        ai, aj, ak = -ai, -aj, -ak\n",
    "\n",
    "    si, sj, sk = math.sin(ai), math.sin(aj), math.sin(ak)\n",
    "    ci, cj, ck = math.cos(ai), math.cos(aj), math.cos(ak)\n",
    "    cc, cs = ci*ck, ci*sk\n",
    "    sc, ss = si*ck, si*sk\n",
    "\n",
    "    M = np.identity(3)\n",
    "    if repetition:\n",
    "        M[i, i] = cj\n",
    "        M[i, j] = sj*si\n",
    "        M[i, k] = sj*ci\n",
    "        M[j, i] = sj*sk\n",
    "        M[j, j] = -cj*ss+cc\n",
    "        M[j, k] = -cj*cs-sc\n",
    "        M[k, i] = -sj*ck\n",
    "        M[k, j] = cj*sc+cs\n",
    "        M[k, k] = cj*cc-ss\n",
    "    else:\n",
    "        M[i, i] = cj*ck\n",
    "        M[i, j] = sj*sc-cs\n",
    "        M[i, k] = sj*cc+ss\n",
    "        M[j, i] = cj*sk\n",
    "        M[j, j] = sj*ss+cc\n",
    "        M[j, k] = sj*cs-sc\n",
    "        M[k, i] = -sj\n",
    "        M[k, j] = cj*si\n",
    "        M[k, k] = cj*ci\n",
    "    return M\n",
    "\n",
    "def matrix_to_euler(matrix, axes='syxz'):\n",
    "    try:\n",
    "        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n",
    "    except (AttributeError, KeyError):\n",
    "        _TUPLE2AXES[axes]  # validation\n",
    "        firstaxis, parity, repetition, frame = axes\n",
    "\n",
    "    i = firstaxis\n",
    "    j = _NEXT_AXIS[i+parity]\n",
    "    k = _NEXT_AXIS[i-parity+1]\n",
    "\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    if repetition:\n",
    "        sy = math.sqrt(M[i, j]*M[i, j] + M[i, k]*M[i, k])\n",
    "        if sy > _EPS:\n",
    "            ax = math.atan2( M[i, j],  M[i, k])\n",
    "            ay = math.atan2( sy,       M[i, i])\n",
    "            az = math.atan2( M[j, i], -M[k, i])\n",
    "        else:\n",
    "            ax = math.atan2(-M[j, k],  M[j, j])\n",
    "            ay = math.atan2( sy,       M[i, i])\n",
    "            az = 0.0\n",
    "    else:\n",
    "        cy = math.sqrt(M[i, i]*M[i, i] + M[j, i]*M[j, i])\n",
    "        if cy > _EPS:\n",
    "            ax = math.atan2( M[k, j],  M[k, k])\n",
    "            ay = math.atan2(-M[k, i],  cy)\n",
    "            az = math.atan2( M[j, i],  M[i, i])\n",
    "        else:\n",
    "            ax = math.atan2(-M[j, k],  M[j, j])\n",
    "            ay = math.atan2(-M[k, i],  cy)\n",
    "            az = 0.0\n",
    "\n",
    "    if parity:\n",
    "        ax, ay, az = -ax, -ay, -az\n",
    "    if frame:\n",
    "        ax, az = az, ax\n",
    "    return ax, ay, az\n",
    "\n",
    "def normalize_angle_delta(angle):\n",
    "    if(angle > np.pi):\n",
    "        angle = angle - 2 * np.pi\n",
    "    elif(angle < -np.pi):\n",
    "        angle = 2 * np.pi + angle\n",
    "    return angle\n",
    "\n",
    "\n",
    "def get_ground_6d_poses(p):\n",
    "    \"\"\" For 6dof pose representaion \"\"\"\n",
    "    pos = np.array([p[3],p[7], p[11]])\n",
    "    R = np.array([[p[0], p[1], p[2]], [p[4], p[5], p[6]], [p[8], p[9], p[10]]])\n",
    "    #angles = matrix_to_euler_2(R)\n",
    "    angles = matrix_to_euler(R)\n",
    "    #angles = rotationMatrixToEulerAngles(R)\n",
    "    return [np.concatenate((pos, angles)),R.reshape(3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kitti ground truth matrix (3x4) to relative pose sequence conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_path(dps):\n",
    "    N=len(dps)\n",
    "    paths=np.zeros((N,no_of_pose),dtype=np.float64)\n",
    "    _dps=[dp[0] for dp in dps]\n",
    "    for i,dp in enumerate(_dps):\n",
    "        if i>0: paths[i]=paths[i-1]+(dp)\n",
    "    return paths\n",
    "\n",
    "def get_labels_rel_poses(pose_seqs):\n",
    "    if time_step>2 :rel_pose_seqs=np.zeros((pose_seqs.shape[0],pose_seqs.shape[1]-1,no_of_pose),dtype=np.float64)\n",
    "    else : rel_pose_seqs=np.zeros((pose_seqs.shape[0],no_of_pose),dtype=np.float64)\n",
    "    for i,seq in enumerate(pose_seqs):\n",
    "        rel_poses=np.zeros((time_step,no_of_pose),dtype=np.float64)\n",
    "        for j,points in enumerate(seq):\n",
    "                pose,R=get_ground_6d_poses(points)\n",
    "                if j==0:\n",
    "                    R_offset=R.T\n",
    "                    trans_offset=pose\n",
    "                    rel_poses[j,:]=pose\n",
    "                else:\n",
    "                    pose-=trans_offset\n",
    "                    pose[:3]=R_offset.dot(pose[:3])\n",
    "                    rel_poses[j,:]=pose\n",
    "        rel_poses[2:]=rel_poses[2:]-rel_poses[1:-1]\n",
    "        for j,pose in enumerate(rel_poses):\n",
    "            rel_poses[j,3:]=[normalize_angle_delta(ang) for ang in pose[3:]] \n",
    "            if _keep_yaw_only==True: rel_poses[j,3:]=[rel_poses[j,3],0,0]\n",
    "        if time_step>2 :rel_pose_seqs[i,:,:]=rel_poses[1:,:].copy()   \n",
    "        else :rel_pose_seqs[i,:]=rel_poses[1,:].copy()   \n",
    "    return rel_pose_seqs\n",
    "\n",
    "def get_labels_seq(label_root1,_files,_overlap=1):\n",
    "    cords=[]\n",
    "    poses=[]\n",
    "    indx={}\n",
    "    label_root2='*txt'\n",
    "    #if test==True: overlap=time_step-1\n",
    "    for start_point in start_points:\n",
    "        for i in _files:\n",
    "            for path in glob.glob(label_root1+str(i).zfill(2)+label_root2):\n",
    "                    f=open(path)\n",
    "                    start=len(poses)\n",
    "                    pose_queue=deque(maxlen=time_step)\n",
    "                    _cords=[]\n",
    "                    for j,line in enumerate(list(f)[start_point:-1]):\n",
    "                        points=np.array([float(point) for point in line.split()])\n",
    "                        _cords.append(points)\n",
    "                        pose_queue.append(points)\n",
    "                        if len(pose_queue)==time_step:\n",
    "                                poses.append(list(pose_queue))\n",
    "                                pose_queue=deque(list(pose_queue)[-_overlap:],maxlen=time_step)  \n",
    "                    #poses=poses[:-1]\n",
    "                    end=len(poses)\n",
    "                    indx[i]=[start,end]\n",
    "                    f.close()\n",
    "                    cords+=_cords\n",
    "    cords=np.array(cords)\n",
    "    poses=np.array(poses)\n",
    "    return cords,poses,indx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cords,abs_train_label,train_indx=get_labels_seq(label_root,train_files,_overlap=overlap)\n",
    "val_cords,abs_val_label,val_indx=get_labels_seq(label_root,val_files,_overlap=overlap)\n",
    "test_cords,abs_test_label,test_indx=get_labels_seq(label_root,test_files,_overlap=(time_step-1))\n",
    "\n",
    "\n",
    "train_label=get_labels_rel_poses(abs_train_label)\n",
    "val_label=get_labels_rel_poses(abs_val_label)\n",
    "test_label=get_labels_rel_poses(abs_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_abs_pose_tds(__label):\n",
    "    _label=__label.copy()\n",
    "    cords=np.zeros((_label.shape[0],no_of_pose))\n",
    "    for i,label in enumerate(_label):\n",
    "        if i==0 :cords[i]+=label[-1]\n",
    "        else:\n",
    "            #rot_mat=euler_to_matrix_2([cords[i-1,3],0,0])\n",
    "            rot_mat=euler_to_matrix(cords[i-1,3],0,0)\n",
    "            last_pose=label[-1]\n",
    "            last_pose[:3]=rot_mat.dot(last_pose[:3])\n",
    "            last_pose+=cords[i-1]\n",
    "            last_pose[3] = (last_pose[3] + np.pi) % (2 * np.pi) - np.pi\n",
    "            cords[i]=last_pose\n",
    "    return cords.copy()\n",
    "def get_label_abs_pose(__label):\n",
    "    _label=__label.copy()\n",
    "    cords=np.zeros((_label.shape[0],no_of_pose))\n",
    "    for i,label in enumerate(_label):\n",
    "        if i==0 :cords[i]+=label\n",
    "        else:\n",
    "            #rot_mat=euler_to_matrix([cords[i-1,3],0,0])\n",
    "            rot_mat=euler_to_matrix(cords[i-1,3],0,0)\n",
    "            last_pose=label\n",
    "            last_pose[:3]=rot_mat.dot(last_pose[:3])\n",
    "            last_pose+=cords[i-1]\n",
    "            last_pose[3] = (last_pose[3] + np.pi) % (2 * np.pi) - np.pi\n",
    "            cords[i]=last_pose\n",
    "    return cords.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean=train_label.mean(axis=0)\n",
    "_std=train_label.std(axis=0)\n",
    "_min=train_label.min(axis=0)\n",
    "_max=train_label.max(axis=0)\n",
    "_diff=(_max - _min)\n",
    "if label_scale=='diff_div': _sub= 0;   _div=_diff\n",
    "if label_scale=='diff_scl': _sub=_mean;_div=_diff\n",
    "if label_scale=='norm_scl': _sub=_mean;_div=_std\n",
    "\n",
    "def scl_label(_label):\n",
    "    _label=(_label-_sub)/_div#[5,1,1.25,20,1,1]\n",
    "    return _label\n",
    "def unscl_label(_label):\n",
    "    _label=(_label*_div)+_sub\n",
    "    return _label\n",
    "train_label=scl_label(train_label)\n",
    "val_label=scl_label(val_label)\n",
    "test_label=scl_label(test_label)\n",
    "#_train_label=unscl_label(_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=1,ncols=len(test_files),figsize=(15,3))\n",
    "for i,[kk,vv] in enumerate(test_indx.items()):\n",
    "    abs_label=get_label_abs_pose(unscl_label(test_label[vv[0]:vv[1]]))\n",
    "    axes[i].plot(abs_label[:,0],abs_label[:,2])\n",
    "    axes[i].set_title('test_'+str(kk))\n",
    "\n",
    "fig,axes=plt.subplots(nrows=2,ncols=3,figsize=(15,5))\n",
    "for i in range(no_of_pose):\n",
    "    if time_step>2 :axes[i//3,i%3].plot(train_label[:,-1,i])\n",
    "    else : axes[i//3,i%3].plot(train_label[: ,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating train image path sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(img_root1,_files,_overlap=1):\n",
    "    img_data_a=[]\n",
    "    img_data_b=[]\n",
    "    img_root2='\\\\image_2\\\\*'\n",
    "    for start_point in start_points:\n",
    "        for i in _files:\n",
    "           img_q=deque(maxlen=time_step)\n",
    "           for path in glob.glob(img_root1+str(i).zfill(2)+img_root2)[start_point:-1]:\n",
    "                    img_q.append(path)\n",
    "                    if len(img_q)==time_step:\n",
    "                       img_data_a.append(list(img_q))\n",
    "                       img_q=deque(list(img_q)[-_overlap:],maxlen=time_step)\n",
    "\n",
    "    return np.array(img_data_a)\n",
    "train_data=get_image_paths(img_root,train_files,_overlap=overlap)\n",
    "val_data=get_image_paths(img_root,val_files,_overlap=overlap)\n",
    "test_data=get_image_paths(img_root,test_files,_overlap=(time_step-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating data high rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,5))\n",
    "plt.plot(train_label[:,3])\n",
    "grads=np.gradient(train_label[:,3])\n",
    "zc = np.diff(np.sign(grads))\n",
    "\n",
    "def rolling_mean(_arr,_win):\n",
    "    _d=_win//2\n",
    "    n_arr=_arr.copy()\n",
    "    __arr=_arr.copy()\n",
    "    for i in range(len(__arr)):\n",
    "        if i>_d:\n",
    "            n_arr[i]=__arr[i-_d:i+_d].mean()\n",
    "    return n_arr\n",
    "_zc=rolling_mean(abs(zc),75)\n",
    "plt.plot(_zc<0.3)\n",
    "plt.plot(_zc)\n",
    "plt.axhline(y=0.3,color='r')\n",
    "plt.show()\n",
    "fig=plt.figure(figsize=(20,5))\n",
    "repeat_index=[False]+list(_zc<0.3)\n",
    "plt.plot(train_label[repeat_index,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repeat_data==True:\n",
    "    train_label=np.concatenate([train_label,train_label[repeat_index]],axis=0)\n",
    "    train_data=np.concatenate([train_data,train_data[repeat_index]],axis=0)\n",
    "train_label.shape,train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[:2],'\\n',test_data[:2])\n",
    "print(len(train_data),len(train_label),len(test_data),len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'img_path':list(train_data),'label':list(train_label)})\n",
    "df.to_csv(log_dir+exp_name+'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=np.arange(len(train_data))\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(train_index)\n",
    "val_index=np.arange(len(val_data))\n",
    "#np.random.seed(seed)\n",
    "#np.random.shuffle(val_index)\n",
    "\n",
    "if load_random_vars==True:\n",
    "    with open(random_vars_dir+'train_index.pkl','rb') as file:\n",
    "         train_index=pickle.load(file)\n",
    "            \n",
    "if not os.path.exists(log_dir+exp_name+'vars/'): os.mkdir(log_dir+exp_name+'vars/')\n",
    "with open(log_dir+exp_name+'vars/train_index.pkl','wb') as file:\n",
    "     pickle.dump(train_index,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_data=[]\n",
    "_train_label=[]\n",
    "for i in train_index:\n",
    "    _train_data.append(train_data[i])\n",
    "    _train_label.append(train_label[i])\n",
    "_train_label=np.array(_train_label)\n",
    "_val_data=[]\n",
    "_val_label=[]\n",
    "for i in val_index:\n",
    "    _val_data.append(val_data[i])\n",
    "    _val_label.append(val_label[i])\n",
    "_val_label=np.array(_val_label)\n",
    "len(_train_label),len(_val_label),len(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hsv2bgr(hsv,scl=2):\n",
    "    out=np.zeros((image_h//2,image_w//2,3),dtype=np.uint8)\n",
    "    out[:,:,0]=hsv[:,:,0]*255\n",
    "    out[:,:,1]=255\n",
    "    out[:,:,2]=hsv[:,:,1]*255\n",
    "    bgr=cv2.cvtColor(out,cv2.COLOR_HSV2BGR)\n",
    "    return bgr\n",
    "\n",
    "def bgr2hsv(bgr,scl=2):\n",
    "    out=np.zeros((image_h//2,image_w//2,3),dtype=np.float32)\n",
    "    out[:,:,0]=bgr[:,:,0]/255.0\n",
    "    out[:,:,1]=1.0\n",
    "    out[:,:,2]=bgr[:,:,1]/255.0\n",
    "    bgr=cv2.cvtColor(out,cv2.COLOR_HSV2BGR)\n",
    "    return bgr\n",
    "\n",
    "\n",
    "def img2flow_vect(image,scl=2,_format='hsv'):\n",
    "    image=cv2.resize(image,(image.shape[1]//scl,image.shape[0]//scl))\n",
    "    if _format=='bgr':\n",
    "        hsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "        hue=hsv[:,:,0]\n",
    "        val=hsv[:,:,2]\n",
    "    if _format=='hsv' :\n",
    "        hue=hsv[:,:,0]\n",
    "        val=hsv[:,:,2]\n",
    "    ang=(hue/255)*2*np.pi\n",
    "    vel=val/255\n",
    "    fx = vel*np.cos(ang)\n",
    "    fy = vel*np.sin(ang)\n",
    "    flow=np.zeros((image.shape[0],image.shape[1],2),dtype=np.float64)\n",
    "    flow[:,:,0]=fx#np.uint8(of_img[:,:,0]*180)\n",
    "    flow[:,:,1]=fy#np.uint8(of_img[:,:,1]*255)\n",
    "    \n",
    "    return flow\n",
    "\n",
    "def flow_vect2img(image,_format='hsv'):\n",
    "    fx=image[:,:,0]\n",
    "    fy=image[:,:,1]\n",
    "    ang = np.arctan2(fy, fx)# + np.pi\n",
    "    vel = np.sqrt(fx*fx+fy*fy)#/np.sqrt(2)\n",
    "    ang=(ang/(2*np.pi))\n",
    "    ang=np.clip(ang,0,0.5)\n",
    "\n",
    "    if _format=='hsv':\n",
    "        out=np.zeros((image.shape[0],image.shape[1],2),dtype=np.float32)\n",
    "        out[:,:,0]=ang\n",
    "        out[:,:,1]=vel\n",
    "        #out[:,:,1]=cv2.normalize(vel,None,0,255,cv2.NORM_MIMMAX)\n",
    "        \n",
    "    if _format=='bgr':\n",
    "        out=np.zeros((image.shape[0],image.shape[1],3),dtype=np.uint8)\n",
    "        out[:,:,0]=ang*255\n",
    "        out[:,:,1]=255\n",
    "        out[:,:,2]=255*vel\n",
    "        out=cv2.cvtColor(out,cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def get_flow_path(_path):\n",
    "        _path=_path.replace('sequences','flow')\n",
    "        _path=_path.replace('png','flow')\n",
    "        _path=_path.replace('\\\\image_2','')\n",
    "        return _path\n",
    "\n",
    "def get_disp_path(_path):\n",
    "        _path=_path.replace('sequences','disp')\n",
    "        _path=_path.replace('png','disp')\n",
    "        _path=_path.replace('\\\\image_2','')\n",
    "        return _path\n",
    "    \n",
    "def get_seg_path(_path):\n",
    "        _path=_path.replace('sequences','seg')\n",
    "        _path=_path.replace('png','seg')\n",
    "        _path=_path.replace('\\\\image_2','')\n",
    "        return _path\n",
    "    \n",
    "def get_flow_vect(img1,img2):\n",
    "    hsv = np.zeros_like(img1)\n",
    "    hsv[...,1] = 255\n",
    "    _img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "    _img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(_img1,_img2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return flow\n",
    "\n",
    "def get_flow_hsv(img1,img2):\n",
    "    hsv = np.zeros_like(img1)\n",
    "    hsv[...,1] = 255\n",
    "    _img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "    _img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(_img1,_img2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    return hsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_scl(image,scl):\n",
    "    of_img=cv2.resize(image,(image_w//scl,image_h//scl))\n",
    "    return of_img\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    if _img_norm==True:\n",
    "        #mean = (-0.1228108650837056, -0.12498114858829212, -0.1423079117403446)\n",
    "        #std =  (0.3224294024021682, 0.3199054482966101, 0.31869854090215954)\n",
    "        mean = (-0.19007764876619865, -0.15170388157131237, -0.10659445665650864)\n",
    "        std =  (0.2610784009469139, 0.25729316928935814, 0.25163823815039915)\n",
    "        img[:,:,0]=(img[:,:,0]-mean[0])/std[0]\n",
    "        img[:,:,1]=(img[:,:,1]-mean[1])/std[1]\n",
    "        img[:,:,2]=(img[:,:,2]-mean[2])/std[2]\n",
    "    \n",
    "    return img.copy()\n",
    "    \n",
    "def batch_generator(_data,_label):\n",
    "    no_of_batch=len(_data)//batch_size\n",
    "    batch_index=np.arange(no_of_batch)\n",
    "    #np.random.seed(seed)\n",
    "    #np.random.shuffle(batch_index)\n",
    "    if time_step>2 :\n",
    "        batch_data_a=np.zeros((batch_size,image_h,image_w,3))\n",
    "        batch_data_b=np.zeros((batch_size,image_h,image_w,3))\n",
    "        batch_flow=np.zeros((batch_size,time_step-1,image_h//2,image_w//2,_flow_chanel))\n",
    "    else:\n",
    "        batch_data_a=np.zeros((batch_size,image_h,image_w,3))\n",
    "        batch_data_b=np.zeros((batch_size,image_h,image_w,3))\n",
    "        batch_flow_1=np.zeros((batch_size,image_h//_flow_scale,image_w//_flow_scale,_flow_chanel))\n",
    "        #batch_flow_2=np.zeros((batch_size,image_h//(_flow_scale*2),image_w//(_flow_scale*2),_chanel))\n",
    "        #batch_flow_3=np.zeros((batch_size,image_h//(_flow_scale*4),image_w//(_flow_scale*4),_chanel))\n",
    "        #batch_flow_4=np.zeros((batch_size,image_h//(_flow_scale*8),image_w//(_flow_scale*8),_chanel))\n",
    "        #batch_flow_5=np.zeros((batch_size,image_h//(_flow_scale*16),image_w//(_flow_scale*16),_chanel))\n",
    "        \n",
    "        batch_disp_1=np.zeros((batch_size,image_h//_disp_scale,image_w//_disp_scale,1))\n",
    "        \n",
    "    while(True):\n",
    "        for ind in batch_index:#range(no_of_batch):\n",
    "            k=0\n",
    "            for j in range(ind*batch_size,(ind+1)*batch_size):\n",
    "                imgs=[]\n",
    "                for p,fname in enumerate(_data[j]):\n",
    "                    img_a=cv2.imread(fname)\n",
    "                    img_a=cv2.resize(img_a,(image_w,image_h))\n",
    "                    imgs.append(img_a)\n",
    "                    if p>0:\n",
    "                        flow_path=get_flow_path(str(_data[j][0]))\n",
    "                        with open(flow_path,'rb') as file:\n",
    "                            of_img=pickle.load(file)\n",
    "                            if _flow_decompress==True:\n",
    "                                of_img=zlib.decompress(of_img)\n",
    "                                of_img=np.frombuffer(of_img,dtype=np.uint8).reshape(192,640,2)\n",
    "                                of_img=(of_img/120-1)\n",
    "                        \n",
    "                        if _disp_or_seg=='disp': disp_path=get_disp_path(str(_data[j][0]))\n",
    "                        if _disp_or_seg=='seg' : disp_path=get_seg_path(str(_data[j][0]))\n",
    "                        with open(disp_path,'rb') as file:\n",
    "                            disp_img=pickle.load(file)\n",
    "                            \n",
    "                                \n",
    "                        #if math.isnan(of_img.mean())==True:\n",
    "                        #    of_img=np.nan_to_num(of_img)\n",
    "                        \n",
    "                        if _flow_format=='hsv' :\n",
    "                            of_img=flow_vect2img(of_img,_format='hsv')\n",
    "                        #if flow_format='vect' :\n",
    "                        #    pass\n",
    "                        of_img=cv2.resize(of_img,(image_w//_flow_scale,image_h//_flow_scale))\n",
    "                        disp_img=cv2.resize(disp_img,(image_w//_disp_scale,image_h//_disp_scale))\n",
    "                        if time_step>2:\n",
    "                            batch_data_a[k,p-1,:,:,:]=normalize_img((imgs[p-1]/255.0)-_img_offset)\n",
    "                            batch_data_b[k,p-1,:,:,:]=normalize_img((imgs[p]/255.0)-_img_offset)\n",
    "                            batch_flow_1[k,p-1,:,:,:]=of_img\n",
    "                        else :\n",
    "                            batch_data_a[k,:,:,:]=(imgs[p-1]/255.0)-_img_offset\n",
    "                            batch_data_b[k,:,:,:]=(imgs[p]/255.0)-_img_offset\n",
    "                            batch_flow_1[k,:,:,:]=of_img\n",
    "                            #batch_flow_2[k,:,:,:]=resize_scl(of_img,_flow_scale*2)\n",
    "                            #batch_flow_3[k,:,:,:]=resize_scl(of_img,_flow_scale*4)\n",
    "                            #batch_flow_4[k,:,:,:]=resize_scl(of_img,_flow_scale*8)\n",
    "                            #batch_flow_5[k,:,:,:]=resize_scl(of_img,_flow_scale*16)\n",
    "                            batch_disp_1[k,:,:,0]=disp_img\n",
    "                \n",
    "                k+=1\n",
    "            batch_trans_label=np.array(_label[ind*batch_size:(ind+1)*batch_size,:3],dtype=np.float32)\n",
    "            batch_rot_label=np.array(_label[ind*batch_size:(ind+1)*batch_size,3:],dtype=np.float32)\n",
    "            yield ([batch_data_a,batch_data_b],[batch_trans_label,batch_rot_label,batch_flow_1,batch_disp_1])#,batch_flow_2,batch_flow_3,batch_flow_4,batch_flow_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = batch_generator(_train_data,_train_label)\n",
    "val_batch   = batch_generator(_val_data,_val_label)\n",
    "grad_batch   = batch_generator(_train_data[:batch_size*_grad_data_len],_train_label[:batch_size*_grad_data_len])\n",
    "sample=next(train_batch)\n",
    "\n",
    "for smp in sample[1]:\n",
    "    print(smp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing some random bathces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_step>2 :seq=sample[0][0][0] #data,data1,batch_1\n",
    "else : seq=sample[0]\n",
    "fig,axes=plt.subplots(nrows=len(seq[0]),ncols=4,figsize=(15,15))\n",
    "labels=sample[1][2:]\n",
    "for i,[img_a,img_b,flow,disp] in enumerate(zip(seq[0],seq[1],labels[0],labels[1])):\n",
    "    if _flow_format=='hsv' :\n",
    "        bgr=hsv2bgr(flow)\n",
    "    if _flow_format=='vect' :\n",
    "        bgr=flow_vect2img(flow,_format='bgr')\n",
    "    axes[i,0].imshow(img_a+0.5)\n",
    "    axes[i,1].imshow(img_b+0.5)\n",
    "    axes[i,2].imshow(bgr[:,:,:])\n",
    "    axes[i,3].imshow(disp[:,:,0])\n",
    "    #axes[i,3].imshow(depth,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions and model defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_loss_flow(y_true, y_pred):\n",
    "      y_true=tf.cast(y_true,tf.float32)\n",
    "      y_pred=tf.cast(y_pred,tf.float32)\n",
    "    \n",
    "      loss=tf.square(tf.subtract(y_true,y_pred))\n",
    "      abs_loss=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=[1,2,3])\n",
    "      if _flow_loss=='rmse' : loss=tf.sqrt(tf.reduce_mean(loss,axis=[1,2,3]))\n",
    "      if _flow_loss=='mse' : loss=tf.reduce_mean(loss,axis=[1,2,3])\n",
    "      return loss,abs_loss \n",
    "\n",
    "def custom_loss_flow_log(y_true, y_pred):\n",
    "      y_true=tf.cast(y_true,tf.float32)\n",
    "      if _flow_format=='hsv':\n",
    "                      y_true=tf.clip_by_value(tf.cast(y_true,tf.float32),0.0,1.0)\n",
    "                      y_pred=tf.clip_by_value(tf.cast(y_pred,tf.float32),0.0,1.0)\n",
    "                      _add_offset=1\n",
    "      if _flow_format=='vect':\n",
    "                      y_true=tf.clip_by_value(tf.cast(y_true,tf.float32),-1.0,1.0)\n",
    "                      y_pred=tf.clip_by_value(tf.cast(y_pred,tf.float32),-1.0,1.0)\n",
    "                      _add_offset=2\n",
    "      y_true=tf.math.log(y_true+_add_offset)\n",
    "      y_pred=tf.math.log(y_pred+_add_offset)\n",
    "        \n",
    "      loss=tf.square(tf.subtract(y_true,y_pred))\n",
    "      abs_loss=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=[1,2,3])\n",
    "      if _flow_loss=='msle':  loss=tf.reduce_mean(loss,axis=[1,2,3])\n",
    "      if _flow_loss=='rmsle': loss=tf.sqrt(tf.reduce_mean(loss,axis=[1,2,3]))\n",
    "      return loss,abs_loss \n",
    "\n",
    "def custom_loss_disp(y_true, y_pred):\n",
    "      y_true=tf.cast(y_true,tf.float32)\n",
    "      y_pred=tf.cast(y_pred,tf.float32)\n",
    "    \n",
    "      loss=tf.square(tf.subtract(y_true,y_pred))\n",
    "      abs_loss=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=[1,2,3])\n",
    "      if _disp_loss=='rmse' : loss=tf.sqrt(tf.reduce_mean(loss,axis=[1,2,3]))\n",
    "      if _disp_loss=='mse' : loss=tf.reduce_mean(loss,axis=[1,2,3])\n",
    "      return loss\n",
    "    \n",
    "def custom_loss_disp_log(y_true, y_pred):\n",
    "      y_true=tf.cast(y_true,tf.float32)\n",
    "      #y_pred=tf.clip_by_value(tf.cast(y_pred,tf.float32),0,10)\n",
    "      y_true=tf.math.log(y_true+1)\n",
    "      y_pred=tf.math.log(y_pred+1)\n",
    "      loss=tf.square(tf.subtract(y_true,y_pred))\n",
    "      abs_loss=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=[1,2,3])\n",
    "      if _disp_loss=='msle':  loss=tf.reduce_mean(loss,axis=[1,2,3])\n",
    "      if _disp_loss=='rmsle': loss=tf.sqrt(tf.reduce_mean(loss,axis=[1,2,3]))\n",
    "      return loss,abs_loss \n",
    "\n",
    "def custom_loss_trans(y_true,y_pred):\n",
    "    abs_loss_trans=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='mae':\n",
    "        loss_trans=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='mse':\n",
    "        loss_trans=tf.reduce_mean(tf.square(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='rmse':\n",
    "        loss_trans=tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true,y_pred)),axis=-1))\n",
    "    return loss_trans,abs_loss_trans\n",
    "\n",
    "def custom_loss_rot(y_true,y_pred):\n",
    "    abs_loss_rot=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='mae':\n",
    "        loss_rot=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='mse':\n",
    "        loss_rot=tf.reduce_mean(tf.square(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    if _pose_loss=='rmse':\n",
    "        loss_rot=tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true,y_pred)),axis=-1))\n",
    "    return loss_rot,abs_loss_rot\n",
    "\n",
    "N=64//scale\n",
    "def _TDS(_layer):\n",
    "    return _layer\n",
    "\n",
    "if time_dis==True:\n",
    "    TDS=TimeDistributed\n",
    "else :\n",
    "    TDS=_TDS\n",
    "    \n",
    "    \n",
    "    \n",
    "def ConvNet(x,classes=3,i=''):\n",
    "    _convs=[]\n",
    "    _par=_net_par\n",
    "    for i in range(_par._num_shared_layer):\n",
    "        _name='block_'+str(_par._block[i])+'_conv_'+str(_par._nconv[i])\n",
    "        x = TDS(Conv2D(_par._filters[i], (_par._kernels[i], _par._kernels[i]), padding='same',strides=_par._strides[i],kernel_regularizer=_par._kernel_regularizer,kernel_initializer=_par._kernel_initializer, name=_name))(x)\n",
    "        if _par._batch_norm[i]: x = BatchNormalization()(x)\n",
    "        if _par._act[i]==1:  \n",
    "                          if _par._leaky_relu==True: x = LeakyReLU(alpha=0.1)(x)\n",
    "                          if _par._relu==True: x = ReLU()(x)\n",
    "        if _par._drop_out[i]>0.0 : x = Dropout(_par._drop_out[i],seed=_par.drop_seed)(x)\n",
    "        if _par._nconv[i]==2 or _par._block[i]==2: _convs.append(x)\n",
    "    _conv_flow=x\n",
    "    _conv_disp=x\n",
    "    x = TDS(Flatten())(x)\n",
    "    #return x\n",
    "\n",
    "    \n",
    "    \n",
    "    _convs=list(reversed(_convs[:4]))#conv6_1,conv5_1,conv4_1,conv3_1,conv2_1\n",
    "    flows=[]\n",
    "    disps=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "\n",
    "        conv_trans=Conv2DTranspose(_par._deconv_filters[i], (4,4), strides=(2, 2), padding='same', name='conv_trans_flow_'+str(i))(_conv_flow)\n",
    "        if _par._deconv_act[i] :\n",
    "            if _par._leaky_relu==True: conv_trans= LeakyReLU(alpha=0.1)(conv_trans)\n",
    "            if _par._relu==True: conv_trans= ReLU()(conv_trans)\n",
    "        flow = Conv2D(_par._flow_chanel, (3, 3),  padding='same',strides=1, name='conv_flow'+str(i))(_conv_flow)\n",
    "        flow = Conv2DTranspose(_par._flow_chanel, (4,4), strides=(2, 2), padding='same', name='flow_'+str(i))(flow)\n",
    "        flows.append(flow)\n",
    "        #if i==-1: _conv_flow = concatenate([_convs[i],conv_trans], axis = 3)\n",
    "        _conv_flow = concatenate([_convs[i],conv_trans,flows[i]], axis = 3)\n",
    "            \n",
    "        \n",
    "    i+=1\n",
    "    flow = Conv2D(_par._flow_chanel, (3, 3),  padding='same',strides=1, name='conv_flow'+str(i))(_conv_flow)\n",
    "    flow = Conv2DTranspose(_par._flow_chanel, (4,4), strides=(2, 2), padding='same', name='flow_'+str(i))(flow)\n",
    "    flows.append(flow)\n",
    "    flows=list(reversed(flows))\n",
    "    \n",
    "    #return flows,x\n",
    "    \n",
    "    for i in range(4):\n",
    "\n",
    "        conv_trans=Conv2DTranspose(_par._deconv_filters[i], (4,4), strides=(2, 2), padding='same', name='conv_trans_disp'+str(i))(_conv_disp)\n",
    "        if _par._deconv_act[i] :\n",
    "            if _par._leaky_relu==True: conv_trans= LeakyReLU(alpha=0.1)(conv_trans)\n",
    "            if _par._relu==True: conv_trans= ReLU()(conv_trans)\n",
    "        disp = Conv2D(_par._disp_chanel, (3, 3),  padding='same',strides=1, name='conv_disp'+str(i))(_conv_disp)\n",
    "        disp = Conv2DTranspose(_par._disp_chanel, (4,4), strides=(2, 2), padding='same', name='disp_'+str(i))(disp)\n",
    "        disps.append(disp)\n",
    "        #if i==-1: _conv = concatenate([_convs[i],conv_trans], axis = 3)\n",
    "        _conv_disp = concatenate([_convs[i],conv_trans,disps[i]], axis = 3)\n",
    "            \n",
    "        \n",
    "    i+=1\n",
    "    disp = Conv2D(_par._disp_chanel, (3, 3),  padding='same',strides=1, name='conv_disp'+str(i))(_conv_disp)\n",
    "    disp = Conv2DTranspose(_par._disp_chanel, (4,4), strides=(2, 2), padding='same', name='disp_'+str(i))(disp)\n",
    "    disp=ReLU()(disp)\n",
    "    disps.append(disp)\n",
    "    disps=list(reversed(disps))\n",
    "    \n",
    "    #return flows,x\n",
    "    return x,flows[0],disps[0]\n",
    "\n",
    "if _flow_loss=='rmsle' or _flow_loss=='msle': _flow_loss_func=custom_loss_flow_log\n",
    "if _flow_loss=='rmse' or _flow_loss=='mse': _flow_loss_func=custom_loss_flow\n",
    "if _disp_loss=='rmsle' or _disp_loss=='msle': _disp_loss_func=custom_loss_disp_log\n",
    "if _disp_loss=='rmse' or _disp_loss=='mse': _disp_loss_func=custom_loss_disp\n",
    "    \n",
    "loss_funcs_dict={'trans':custom_loss_trans,\n",
    "                 'rot':custom_loss_rot,\n",
    "                 'flow_4':_flow_loss_func,\n",
    "                 'disp_4':_disp_loss_func}\n",
    "                 #'flow_3':custom_loss_flow,\n",
    "                 #'flow_2':custom_loss_flow,\n",
    "                 #'flow_1':custom_loss_flow,\n",
    "                 #'flow_0':custom_loss_flow} \n",
    "\n",
    "\n",
    "def PRF_NET(input_shape=(224,224,2)):\n",
    "    _input_a=Input(shape=input_shape)\n",
    "    _input_b=Input(shape=input_shape) \n",
    "    #_input_c=Input(shape=[input_shape[0]//_depth_scale,input_shape[1]//_depth_scale,3])\n",
    "    #_depth=Input(shape=[input_shape[0]//_depth_scale,input_shape[1]//_depth_scale])\n",
    "    _input=concatenate([_input_a,_input_b],axis=3)\n",
    "    x,flow,disp=ConvNet(_input,i=0)\n",
    "    _par=_net_par\n",
    "    if time_dis==True:\n",
    "               x = LSTM(_par._lstm_size,kernel_initializer=_par._kernel_initializer,recurrent_initializer=_par._kernel_initializer,return_sequences=True )(x)\n",
    "               if _par._rot_drop_out[0]>0.0 : x = Dropout(_par._drop_out[10],seed=_par.drop_seed)(x)\n",
    "               x = LSTM(_par._lstm_size,kernel_initializer=_par.kernel_initializer,recurrent_initializer=_par._kernel_initializer,return_sequences=True)(x)\n",
    "               if _par._rot_drop_out[1]>0.0 : x = Dropout(_par._drop_out[11],seed=_par.drop_seed)(x)\n",
    "    else :\n",
    "               trans=x\n",
    "               for i in range(_par._num_trans_layers):\n",
    "                   trans = Dense(_par._trans_layer_nodes[i],kernel_regularizer=_par._kernel_regularizer, name='dense_1.'+str(i+1))(trans)\n",
    "                   if _par._trans_act[i]==1 :\n",
    "                              if _par._relu==True: trans = ReLU()(trans)\n",
    "                              if _par._leaky_relu==True: trans = LeakyReLU(alpha=0.1)(trans)\n",
    "                   if _par._trans_drop_out[i]>0.0 : trans = Dropout(_par._drop_out[i],seed=_par.drop_seed)(trans)\n",
    "                \n",
    "               trans = Dense(no_of_pose//2,kernel_regularizer=_par._kernel_regularizer, name='trans')(trans)\n",
    "            \n",
    "               rot=x\n",
    "               for i in range(_par._num_rot_layers):\n",
    "                   rot = Dense(_par._rot_layer_nodes[i],kernel_regularizer=_par._kernel_regularizer, name='dense_2.'+str(i+1))(rot)\n",
    "                   if _par._rot_act[i]==1:\n",
    "                                     if _par._relu==True: rot = ReLU()(rot)\n",
    "                                     if _par._leaky_relu==True: rot = LeakyReLU(alpha=0.1)(rot)\n",
    "                   if _par._rot_drop_out[i]>0.0 : rot = Dropout(_par._drop_out[i],seed=_par.drop_seed)(rot)\n",
    "                \n",
    "               rot = Dense(no_of_pose//2,kernel_regularizer=_par._kernel_regularizer, name='rot')(rot)\n",
    "    \n",
    "    model = Model([_input_a,_input_b], [trans,rot,flow,disp], name='network')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "if time_dis==True : model = PRF_NET(input_shape=[time_step-1,image_h, image_w,3])\n",
    "else : model = PRF_NET(input_shape=[image_h, image_w,3])\n",
    "for layer in model.layers:\n",
    "    if 'disp' in layer.name: layer.trainable=_train_disp\n",
    "    if 'flow' in layer.name: layer.trainable=_train_flow\n",
    "    print(layer.name,layer.trainable)\n",
    "model.summary()\n",
    "plot_model(model,to_file=log_dir+exp_name+'model.png')\n",
    "\n",
    "if load_init_wts==True:\n",
    "       model.load_weights(random_vars_dir+'init_weights.hdf5')\n",
    "model.save(log_dir+exp_name+'vars/init_weights.hdf5')\n",
    "\n",
    "        \n",
    "if pre_trained_wts_path !=None: \n",
    "        model.load_weights(pre_trained_wts_path)\n",
    "if pre_trained_opt_path !=None: \n",
    "        #model._make_train_function()\n",
    "        with open(pre_trained_opt_path, 'rb') as f:\n",
    "            curr_optimizer = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_count=0\n",
    "if len(wt_path) != 0:\n",
    "    weight_file = h5py.File(wt_path, 'r')\n",
    "    curr_layer_names=[lyr.name for i,lyr in enumerate(model.layers)]\n",
    "    saved_layer_names=list(weight_file['model_weights'].keys())\n",
    "    \n",
    "    for saved_name in saved_layer_names:\n",
    "        try :shp = weight_file['model_weights'][saved_name][saved_name]['kernel:0'].shape\n",
    "        except: continue\n",
    "        if saved_name in curr_layer_names:\n",
    "            weights=weight_file['model_weights'][saved_name][saved_name]['kernel:0']\n",
    "            biases=weight_file['model_weights'][saved_name][saved_name]['bias:0']\n",
    "            layer=model.get_layer(saved_name)\n",
    "            layer.set_weights([weights,biases])\n",
    "            layer_count+=1\n",
    "            if saved_name[:5] != 'block': layer.trainable = False\n",
    "            print(saved_name,shp,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train==True:\n",
    "    #class loss_history(tf.keras.callbacks.Callback):\n",
    "    class loss_logger():\n",
    "        def on_train_begin(self, logs={}):\n",
    "            if pre_trained_log_path ==None: \n",
    "                self.fe=open(log_dir+exp_name+'loss_epoch.csv','w+')\n",
    "            else : \n",
    "                old_log=open(pre_trained_log_path,'r')\n",
    "                old_log_data = old_log.read()\n",
    "                old_log.close()\n",
    "                self.fe=open(log_dir+exp_name+'loss_epoch.csv','w+')\n",
    "                for line in old_log_data:\n",
    "                        self.fe.write(line)\n",
    "            self.fe.close()\n",
    "            self.columns=[]\n",
    "            \n",
    "            \n",
    "        def on_epoch_begin(self, epoch, logs={}):\n",
    "            self.epoch=epoch            \n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs={},grads={},calculated_grads=[],_curr_opt=[]):\n",
    "            if epoch==1:\n",
    "                loss_str=''\n",
    "                for _item in logs.keys():\n",
    "                    loss_str+=str(_item)+','\n",
    "                    self.columns.append(_item)\n",
    "                loss_str=loss_str[:-1]+'\\n'\n",
    "            else:\n",
    "                loss_str=''\n",
    "                \n",
    "            for _item in self.columns:\n",
    "                loss_str+=str(logs[_item])+','\n",
    "            loss_str=loss_str[:-1]+'\\n'\n",
    "            \n",
    "            with open(log_dir+exp_name+'loss_epoch.csv','a+') as fe:\n",
    "                fe.write(loss_str)\n",
    "            if not os.path.exists(log_dir+exp_name+'calc_grads'): os.mkdir(log_dir+exp_name+'calc_grads')\n",
    "            if not os.path.exists(log_dir+exp_name+'grads'): os.mkdir(log_dir+exp_name+'grads')\n",
    "            \n",
    "            if epoch%_grad_log_epoch==0 or epoch==1:\n",
    "                \n",
    "                _t_wts={};_r_wts={};_f_wts={};\n",
    "                \n",
    "                _t_wts=grads['trans']\n",
    "                _r_wts=grads['rot']\n",
    "                _f_wts=grads['flow_4']\n",
    "                _d_wts=grads['disp_4']\n",
    "                \n",
    "                with open(log_dir+exp_name+'grads\\\\'+'trans_'+str(epoch)+'.grads','wb') as file:\n",
    "                        pickle.dump(_t_wts,file)\n",
    "                with open(log_dir+exp_name+'grads\\\\'+'rot_'+str(epoch)+'.grads','wb') as file:\n",
    "                        pickle.dump(_r_wts,file)\n",
    "                if _train_flow==True:\n",
    "                    with open(log_dir+exp_name+'grads\\\\'+'flow_'+str(epoch)+'.grads','wb') as file:\n",
    "                            pickle.dump(_f_wts,file)\n",
    "                if _train_disp==True:\n",
    "                    with open(log_dir+exp_name+'grads\\\\'+'disp_'+str(epoch)+'.grads','wb') as file:\n",
    "                            pickle.dump(_d_wts,file)\n",
    "                        \n",
    "            if epoch%_calc_grad_log_epoch==0 or epoch==1:\n",
    "                with open(log_dir+exp_name+'calc_grads\\\\'+'calc_grads_'+str(epoch)+'.grads','wb') as file:\n",
    "                        for key in calculated_grads.keys():\n",
    "                            if type(calculated_grads[key]) == list:\n",
    "                                for i in range(len(calculated_grads[key])):\n",
    "                                    if calculated_grads[key][i] != None:\n",
    "                                         calculated_grads[key][i]= calculated_grads[key][i].numpy() \n",
    "                            else:\n",
    "                                calculated_grads[key]= calculated_grads[key].numpy()  \n",
    "                        pickle.dump(calculated_grads,file)\n",
    "                \n",
    "            \n",
    "            #symbolic_weights = getattr(_opt, 'weights')\n",
    "            #weight_values = K.batch_get_value(symbolic_weights)\n",
    "            if not os.path.exists(log_dir+exp_name+'weights'): os.mkdir(log_dir+exp_name+'weights')\n",
    "            if (epoch)%_all_wts_log_epoch==0:\n",
    "                model.save(log_dir+exp_name+'weights\\\\'+str(epoch)+'.hdf5')\n",
    "                if _save_opts==True:\n",
    "                    if not os.path.exists(log_dir+exp_name+'opts'): os.mkdir(log_dir+exp_name+'opts')\n",
    "                    with open(log_dir+exp_name+'opts\\\\'+str(epoch)+'.pkl', 'wb') as f:\n",
    "                            pickle.dump(_curr_opt, f)\n",
    " \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi task learning with Auxiliary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_main(_images, _labels,_task):\n",
    "#def train_step_main(_images, _labels,_loss_dict,_std_dict,_grad_dict,_task,total_gradients):\n",
    "    _mtp=train_params_tracker(_task)\n",
    "    _loss_funcs=list(loss_funcs_dict.keys())\n",
    "    _n=_loss_funcs.index(_task)\n",
    "    with tf.GradientTape() as tape:\n",
    "        _predictions = model(_images, training=True)\n",
    "        lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "\n",
    "        loss_func=loss_funcs_dict[loss_name]\n",
    "        loss_weight=loss_weights_dict[loss_name]\n",
    "\n",
    "        _batch_loss,_abs_loss=loss_func(lbl, pred)\n",
    "        _std_loss=tf.math.reduce_std(_abs_loss)\n",
    "        _abs_loss=tf.math.reduce_mean(_abs_loss)\n",
    "        \n",
    "        _loss = tf.reduce_mean(_batch_loss)\n",
    "        _gradients = tape.gradient(_loss, model.trainable_variables) \n",
    "        \n",
    "        _mtp._loss_dict[loss_name+'_loss']=_loss\n",
    "        _mtp._loss_dict['train_loss']=loss_weight*_loss\n",
    "        _mtp._std_dict[loss_name+'_std'] = _std_loss \n",
    "        _mtp._grad_dict[loss_name+'_grad'] = _gradients\n",
    "        _mtp._grad_dict[loss_name+'_abs_loss'] = _abs_loss\n",
    "    del tape\n",
    "    for i in range(len(_mtp.total_gradients)):\n",
    "        if _gradients[i]!=None: _mtp.total_gradients[i]= tf.multiply(loss_weight,_gradients[i])\n",
    "\n",
    "    return [_mtp.__dict__]\n",
    "\n",
    "class slope_handler():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rel_loss_logger=np.zeros((epochs+1,len(_init_loss_wts)),dtype=np.float32)\n",
    "        self.rel_reward_logger=np.zeros((epochs+1,len(_init_loss_wts)),dtype=np.float32)\n",
    "        self.rel_loss_1st_grad_logger=np.ones((epochs+1,len(_init_loss_wts)),dtype=np.float32)     \n",
    "        self.rel_loss_2nd_grad_logger=np.ones((epochs+1,len(_init_loss_wts)),dtype=np.float32)\n",
    "        self.prev_rel_loss=np.ones(len(_init_loss_wts),dtype=np.float32)\n",
    "        self.prev_rel_reward=np.zeros(len(_init_loss_wts),dtype=np.float32)\n",
    "        \n",
    "        self._rel_loss={}\n",
    "        self._rel_reward={}\n",
    "        self._1st_grad={}\n",
    "        self._2nd_grad={}\n",
    "        self.update(init=True)\n",
    "        \n",
    "    def get_nth_grad(self,nth,_epoch):\n",
    "        _grad=self.rel_reward_logger[:_epoch+1]\n",
    "        for i in range(nth):\n",
    "            _grad=np.gradient(_grad,axis=0)\n",
    "            #if (i+1)%2 !=0:\n",
    "            #    _grad=-1*_grad\n",
    "        _grad=_grad+_mtl_loss_slope_center\n",
    "        return _grad\n",
    "    \n",
    "    def update(self,_epoch=1,_curr_rel_loss=[],init=False):\n",
    "        if init==True:\n",
    "            for loss_name in loss_funcs_dict.keys():\n",
    "                self._rel_loss[loss_name]=_mtl_loss_center+0.5\n",
    "                self._rel_reward[loss_name]=0.0\n",
    "                self._1st_grad[loss_name]=1.0\n",
    "                self._2nd_grad[loss_name]=1.0\n",
    "            \n",
    "        else:\n",
    "            curr_rel_reward=1-np.array(list(_curr_rel_loss.values()))\n",
    "            self.rel_reward_logger[_epoch,:]=_mtl_loss_slope_alpha*curr_rel_reward+(1-_mtl_loss_slope_alpha)*self.prev_rel_reward\n",
    "            self.prev_rel_reward=self.rel_reward_logger[_epoch,:]\n",
    "            \n",
    "            curr_rel_loss=np.array(list(_curr_rel_loss.values()))\n",
    "            self.rel_loss_logger[_epoch,:]=_mtl_loss_slope_alpha*curr_rel_loss+(1-_mtl_loss_slope_alpha)*self.prev_rel_loss\n",
    "            self.prev_rel_loss=self.rel_loss_logger[_epoch,:]\n",
    "            \n",
    "            #Getting Each values for current epoch\n",
    "            self.rel_loss_logger[_epoch,:]=self.rel_loss_logger[_epoch,:]+(_mtl_loss_center-0.5)\n",
    "            #self.rel_reward_logger[_epoch,:]=self.rel_reward_logger[_epoch,:]\n",
    "            self.rel_loss_1st_grad_logger=self.get_nth_grad(1,_epoch)\n",
    "            self.rel_loss_2nd_grad_logger=self.get_nth_grad(2,_epoch)\n",
    "            \n",
    "            #Saving each values for current epoch for using in current training step\n",
    "            for loss_ind,loss_name in enumerate(list(loss_funcs_dict.keys())):\n",
    "                self._rel_loss[loss_name]=self.rel_loss_logger[_epoch,loss_ind]\n",
    "                self._rel_reward[loss_name]=self.rel_reward_logger[_epoch,loss_ind]\n",
    "                self._1st_grad[loss_name]=self.rel_loss_1st_grad_logger[_epoch,loss_ind]\n",
    "                self._2nd_grad[loss_name]=self.rel_loss_2nd_grad_logger[_epoch,loss_ind]\n",
    "                   \n",
    "            \n",
    "        \n",
    "        \n",
    "    def log(self,_epoch=1):\n",
    "        loss_slope_dict=loss_dict_obj()\n",
    "        for loss_ind,loss_name in enumerate(loss_funcs_dict.keys()):\n",
    "                loss_slope_dict[loss_name+'_0th_reward']=self.rel_reward_logger[_epoch,loss_ind]\n",
    "                loss_slope_dict[loss_name+'_0th_loss']=self.rel_loss_logger[_epoch,loss_ind]\n",
    "                loss_slope_dict[loss_name+'_1st_grad']=self.rel_loss_1st_grad_logger[_epoch,loss_ind]\n",
    "                loss_slope_dict[loss_name+'_2nd_grad']=self.rel_loss_2nd_grad_logger[_epoch,loss_ind]\n",
    "        return loss_slope_dict\n",
    "    \n",
    "def cosine_similarity_tf(x,y):\n",
    "    dot_prod=tf.reduce_sum(x*y)\n",
    "    mag_x=tf.sqrt(tf.reduce_sum(tf.square(x)))\n",
    "    mag_y=tf.sqrt(tf.reduce_sum(tf.square(y)))\n",
    "    _res=tf.truediv(dot_prod,tf.multiply(mag_x,mag_y))\n",
    "    #_res=tf.math.acos(_res)\n",
    "    return _res\n",
    "\n",
    "class cosine_handler():\n",
    "        def __init__(self):\n",
    "            self.cosines={}\n",
    "            self.update()\n",
    "                \n",
    "        def update(self,_curr_cosine=None):\n",
    "            for loss_name in (loss_funcs_dict.keys()):\n",
    "                if _curr_cosine==None:\n",
    "                    self.cosines[loss_name]=tf.ones((len(model.trainable_variables)),dtype=tf.float32)\n",
    "                else:\n",
    "                    try: self.cosines[loss_name]=_curr_cosine[loss_name]\n",
    "                    except KeyError: pass\n",
    "                    \n",
    "        def get_name(self,layer_name,loss_name):\n",
    "            _name=layer_name[:-2].replace('/','_')\n",
    "            _name=_name.replace('block','layer')\n",
    "            _name=_name.replace('_conv_','')\n",
    "            _name=loss_name+'_'+_name\n",
    "            return _name\n",
    "        \n",
    "        def log(self,_curr_cosines):\n",
    "            cosine_log_dict=loss_dict_obj()\n",
    "            for loss_ind,loss_name in enumerate(list(loss_funcs_dict.keys())[2:]):\n",
    "                    for layer_name in grad_log_layers:\n",
    "                        if layer_name[:5]=='block':\n",
    "                            layer_ind=layer_name_dict[layer_name]\n",
    "                            _name=self.get_name(layer_name,loss_name)\n",
    "                            cosine_log_dict[_name]=self.cosines[loss_name][layer_ind].numpy()\n",
    "                            \n",
    "            return cosine_log_dict\n",
    "        \n",
    "@tf.function        \n",
    "def get_grad_var(_images, _labels,_task):\n",
    "        _loss_funcs=list(loss_funcs_dict.keys())\n",
    "        _n=_loss_funcs.index(_task)\n",
    "        \n",
    "        _grad_variance=[None]*36\n",
    "        _all_grads=[None]*batch_size\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape: \n",
    "            _predictions = model(_images, training=True)\n",
    "\n",
    "            lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "            loss_func=loss_funcs_dict[loss_name]\n",
    "\n",
    "            _batch_loss,_abs_loss_b=loss_func(lbl, pred)\n",
    "            _loss_b = tf.reduce_mean(_batch_loss)\n",
    "\n",
    "            _grads_m=tape.gradient(_loss_b, model.trainable_variables[:36])\n",
    "            \n",
    "            for k in range(batch_size):\n",
    "                _grads=tape.gradient(_batch_loss[k], model.trainable_variables[:36]) \n",
    "                _all_grads[k]=_grads\n",
    "                \n",
    "        del tape                      \n",
    "        for _grads in _all_grads:\n",
    "            for k,(_g1,_g2) in enumerate(zip(_grads_m,_grads)):\n",
    "                if _grad_variance[k] != None : _grad_variance[k]=_grad_variance[k]+(1/batch_size)*tf.square(_g1-_g2)\n",
    "                else : _grad_variance[k]= (1/batch_size)*tf.square(_g1-_g2)\n",
    "                    \n",
    "        return _grad_variance\n",
    "            \n",
    "@tf.function\n",
    "def train_step_aux(_images, _labels,_task,_mtp,_grad_var_dict):\n",
    "        _atp=train_params_tracker(_task)\n",
    "        _loss_funcs=list(loss_funcs_dict.keys())\n",
    "        _main_task=_loss_funcs[_mtl_main_task_index]\n",
    "        _n=_loss_funcs.index(_task)\n",
    "        _std_loss_a=_mtp['_std_dict'][_main_task+'_std']\n",
    "        _abs_loss_a=_mtp['_grad_dict'][_main_task+'_abs_loss']\n",
    "        _gradients=_mtp['_grad_dict'][_main_task+'_grad']\n",
    "        _grad_var_a=_grad_var_dict[_main_task]\n",
    "        _grad_var_b=_grad_var_dict[_task]\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            _predictions = model(_images, training=True)\n",
    "\n",
    "            lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "            loss_func=loss_funcs_dict[loss_name]\n",
    "            loss_weight=loss_weights_dict[loss_name]\n",
    "\n",
    "            _batch_loss,_abs_loss_b=loss_func(lbl, pred)\n",
    "            _std_loss_b=tf.math.reduce_std(_abs_loss_b)\n",
    "            _abs_loss_b=tf.math.reduce_mean(_abs_loss_b)\n",
    "\n",
    "            _loss_b = tf.reduce_mean(_batch_loss)\n",
    "            _grads=tape.gradient(_loss_b, model.trainable_variables) \n",
    "\n",
    "            _atp._loss_dict[loss_name+'_loss']=_loss_b\n",
    "            _atp._loss_dict['train_loss']=loss_weight*_loss_b\n",
    "            _atp._std_dict[loss_name+'_std'] = _std_loss_b\n",
    "            _atp._grad_dict[loss_name+'_grad'] = _grads\n",
    "\n",
    "        del tape\n",
    "        if _mtl_aux_grad_algo=='v2' or _mtl_aux_grad_algo=='v3'or _mtl_use_cosine_v4==True:\n",
    "                _cos=[]\n",
    "                for k in range(len(_atp.total_gradients)):\n",
    "                    if _gradients[k] != None and _grads[k] != None:\n",
    "                        g1=tf.reshape(_gradients[k],[-1])\n",
    "                        g2=tf.reshape(_grads[k],[-1])\n",
    "                        _cos.append(cosine_similarity_tf(g1,g2))\n",
    "                    else:\n",
    "                        _cos.append(0.0)\n",
    "\n",
    "                curr_cosine=tf.cast(_cos,dtype=tf.float32)#+1 check\n",
    "                curr_cosine=(curr_cosine-tf.reduce_mean(curr_cosine))/tf.abs(tf.reduce_max(curr_cosine)-tf.reduce_min(curr_cosine))\n",
    "                curr_cosine=_mtl_cosine_use_fact*curr_cosine+_mtl_cos_center\n",
    "                #cosines.update(loss_name,curr_cosine)\n",
    "                if _mtl_cosine_avg=='eqn_1' : _cosines=(1-_mtl_cos_alpha)*cosines.cosines[loss_name]+_mtl_cos_alpha*curr_cosine\n",
    "                if _mtl_cosine_avg=='eqn_2' : _cosines=_mtl_cos_alpha*cosines.cosines[loss_name]+curr_cosine[loss_name]\n",
    "                _atp.updated_cosines[loss_name]=_cosines\n",
    "        _beta_layer_log=[];_beta_node_log=[]\n",
    "        _beta_conv_log=[];_coef_log=[];_var_a_log=[];_var_b_log=[];_grads_b_log=[]\n",
    "        \n",
    "        for k in range(len(_atp.total_gradients)):\n",
    "                if _gradients[k] != None and _grads[k] != None:\n",
    "\n",
    "                        if _mtl_aux_grad_algo=='v1':\n",
    "                                _beta=loss_weight\n",
    "\n",
    "                        \n",
    "                        if _mtl_aux_grad_algo=='v4':\n",
    "                            \n",
    "                                _min_std=10e-20\n",
    "\n",
    "                                if _mtl_get_variance_from=='calc':   \n",
    "                                    _var_a=_grad_var_a[k]+_min_std\n",
    "                                    _var_b=_grad_var_b[k]+_min_std\n",
    "\n",
    "                                if _mtl_use_factors=='default':\n",
    "                                    _ratio_fact=_var_b/_var_a\n",
    "                                    if _mtl_use_std_ratio==True:\n",
    "                                        _ratio_fact=tf.sqrt(_ratio_fact)\n",
    "                                    _coef=tf.square((_gradients[k]-_grads[k]))\n",
    "                                    _coef=_coef/_var_a\n",
    "                                    _coef=tf.exp(-0.5*_coef)\n",
    "                                    _beta_conv=tf.multiply(_ratio_fact,_coef) \n",
    "                                    \n",
    "\n",
    "                                elif _mtl_use_factors=='prob_only':\n",
    "                                    _coef=tf.square((_gradients[k]-_grads[k]))\n",
    "                                    _coef=_coef/_var_a\n",
    "                                    _coef=tf.exp(-0.5*_coef)\n",
    "                                    _std_fact=1/(tf.sqrt(2*np.pi)*tf.sqrt(_var_a))\n",
    "                                    _beta_conv=tf.multiply(_std_fact,_coef) \n",
    "                                    if _mtl_take_log_prob==True: \n",
    "                                        _beta_conv=tf.math.log(_beta_conv+10e-20)\n",
    "\n",
    "                                else:\n",
    "                                    print('Give a valid value for _mtl_use_factors instead of ',_mtl_use_factors)\n",
    "                                    return None\n",
    "\n",
    "                                if _mtl_use_mode_v4=='mode-1' and _mtl_use_factors != 'combine_gaussian':\n",
    "                                    _beta_conv=loss_weight*_beta_conv\n",
    "                                if _mtl_use_cosine_v4==True:\n",
    "                                    _beta_conv=tf.multiply(_beta_layer,_beta_conv)\n",
    "                                \n",
    "                                \n",
    "\n",
    "                                _beta_conv_log.append(_beta_conv)\n",
    "                                _var_a_log.append(_var_a)\n",
    "                                _var_b_log.append(_var_b)\n",
    "                                _coef_log.append(_coef)\n",
    "                                \n",
    "                                if _mtl_beta_norm_method=='min_max': \n",
    "                                    _beta_conv=(_beta_conv-tf.reduce_mean(_beta_conv))/(tf.reduce_max(_beta_conv)-tf.reduce_min(_beta_conv))\n",
    "                                    _beta_conv=_mtl_beta_use_fact*_beta_conv+1.0\n",
    "                                if _mtl_beta_norm_method=='std':     \n",
    "                                    _beta_conv=(_beta_conv-tf.reduce_mean(_beta_conv))/tf.math.reduce_std(_beta_conv)\n",
    "                                    _beta_conv=_mtl_beta_use_fact*_beta_conv+1.0\n",
    "                                    \n",
    "                        if _mtl_aux_grad_algo=='v1':_atp.total_gradients[k]=tf.multiply(_beta,_grads[k])# check ***\n",
    "                        elif _mtl_aux_grad_algo=='v2':_atp.total_gradients[k]=tf.multiply(_beta_layer,_grads[k])# check ***\n",
    "                        elif _mtl_aux_grad_algo=='v3':_atp.total_gradients[k]=tf.multiply(_beta_node,_grads[k])# check ***\n",
    "                        elif _mtl_aux_grad_algo=='v4':_atp.total_gradients[k]=tf.multiply(_beta_conv,_grads[k])# check ***\n",
    "                        elif _mtl_aux_grad_algo=='v4' and _mtl_use_factors=='combine_gaussian':\n",
    "                                 _atp.total_gradients[k] = _gradients[k]+tf.multiply(_beta_conv,(_grads[k]-_gradients[k]))\n",
    "                                 _atp.total_gradients[k] = loss_weight*_atp.total_gradients[k]\n",
    "                        else: print('Give a valid value for _mtl_aux_grad_algo :',_mtl_aux_grad_algo,'or _mtl_use_factors :',_mtl_use_factors)\n",
    "                \n",
    "                elif _grads[k] != None:\n",
    "                     _atp.total_gradients[k]= loss_weight*_grads[k]\n",
    "                 \n",
    "        \n",
    "        \n",
    "        if _mtl_aux_grad_algo=='v2':\n",
    "            _atp._grad_dict['beta_layer_mask']=_beta_layer_log\n",
    "        if _mtl_aux_grad_algo=='v3':\n",
    "            _atp._grad_dict['beta_node_mask']=_beta_node_log\n",
    "        if _mtl_aux_grad_algo=='v4':\n",
    "            _atp._grad_dict[loss_name+'_beta_conv_mask']=_beta_conv_log\n",
    "            _atp._grad_dict[loss_name+'_var_a']=_var_a_log\n",
    "            _atp._grad_dict[loss_name+'_var_b']=_var_b_log\n",
    "            if _mtl_use_factors=='importance_sample':\n",
    "                    _atp._grad_dict[loss_name+'_grad_b']=_grads_b_log\n",
    "            #_atp._grad_dict[loss_name+'_coef']=_coef_log\n",
    "        return [_atp.__dict__]\n",
    "\n",
    "\n",
    "def apply_clipped_grads(total_gradients):\n",
    "      if _grad_clip != 0:\n",
    "        for  _layer in grad_clip_layers.keys():\n",
    "                 _clip_value=grad_clip_layers[_layer]\n",
    "                 ind=layer_name_dict[_layer]\n",
    "                 total_gradients[ind]= tf.clip_by_value(total_gradients[ind],-_clip_value,_clip_value) \n",
    "\n",
    "      curr_optimizer.apply_gradients(zip(total_gradients, model.trainable_variables))\n",
    "      return 1\n",
    "\n",
    "class train_params_tracker():\n",
    "    def __init__(self,_task=None):\n",
    "        self.total_gradients=[None]*len(model.trainable_variables)\n",
    "        self.updated_cosines={}\n",
    "        if _task != None :self.updated_cosines[_task]=tf.ones((len(model.trainable_variables)),dtype=tf.float32)\n",
    "        self._grad_dict={}\n",
    "        self._std_dict={}\n",
    "        self._loss_dict={'train_loss':0.0,'rot_loss':0.0,'trans_loss':0.0,'flow_4_loss':0.0,'disp_4_loss':0.0}\n",
    "        \n",
    "    def update(self,obj):\n",
    "        for i,(gr1,gr2) in enumerate(zip(self.total_gradients,obj['total_gradients'])):\n",
    "            if gr1 ==None: gr1=0.0\n",
    "            if gr2 ==None: gr2=0.0\n",
    "            self.total_gradients[i]=gr1+gr2\n",
    "        for key in self._loss_dict.keys():\n",
    "            self._loss_dict[key]+=obj['_loss_dict'][key]\n",
    "        self.updated_cosines.update(obj['updated_cosines'])\n",
    "        self._grad_dict.update(obj['_grad_dict'])\n",
    "        self._std_dict.update(obj['_std_dict'])\n",
    "    def update_all(self,list_obj):\n",
    "        for obj in list_obj:\n",
    "            self.update(obj)\n",
    "        \n",
    "#@tf.function\n",
    "def train_step_mtl(_images,_labels,_grad_var_dict={}):\n",
    "    _updated_train_params=train_params_tracker()\n",
    "    _train_params=[]      \n",
    "    _train_params+=train_step_main(_images,_labels,'trans')\n",
    "    _train_params+=train_step_main(_images,_labels,'rot')\n",
    "    if _train_flow==True: _train_params+=train_step_aux(_images, _labels,'flow_4',_train_params[_mtl_main_task_index],_grad_var_dict)\n",
    "    if _train_disp==True: _train_params+=train_step_aux(_images, _labels,'disp_4',_train_params[_mtl_main_task_index],_grad_var_dict)\n",
    "    _updated_train_params.update_all(_train_params)\n",
    "    del _train_params\n",
    "    apply_clipped_grads(_updated_train_params.total_gradients)\n",
    "    _grad_dict = _updated_train_params._grad_dict\n",
    "    updated_cosines= _updated_train_params.updated_cosines\n",
    "    _loss_dict = _updated_train_params._loss_dict\n",
    "    tg=_updated_train_params.total_gradients\n",
    "    del _updated_train_params\n",
    "    return _loss_dict,_grad_dict,updated_cosines#_train_params[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traininig Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels,loss_funcs_dict,loss_weights_dict):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    train_loss=0\n",
    "    loss_dict={}\n",
    "    for lbl,pred,loss_name in zip(labels,predictions,loss_funcs_dict.keys()):\n",
    "         loss_func=loss_funcs_dict[loss_name]\n",
    "         loss_weight=loss_weights_dict[loss_name]\n",
    "         _loss,_=loss_func(lbl, pred)\n",
    "         _loss = tf.reduce_mean(_loss)\n",
    "         loss_dict[loss_name]=_loss\n",
    "         train_loss+=loss_weight*_loss\n",
    "  \n",
    "  gradients = tape.gradient(train_loss, model.trainable_variables) \n",
    "    \n",
    "  if _grad_clip != 0:\n",
    "    for i in range(len(gradients)):\n",
    "        if layer_no_dict[i] in list(grad_clip_layers.keys()):\n",
    "             _clip_value=grad_clip_layers[layer_no_dict[i]]\n",
    "             gradients[i]= tf.clip_by_value(gradients[i],-_clip_value,_clip_value) \n",
    "                \n",
    "  curr_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  loss_dict['loss']=train_loss\n",
    "  return loss_dict\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_grads(images, labels,_log_layers):\n",
    "    grad_dict={}\n",
    "    loss_names=list(loss_funcs_dict.keys())\n",
    "    for i in range(4):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss_func=loss_funcs_dict[loss_names[i]]\n",
    "            _loss,_=loss_func(labels[i], predictions[i])\n",
    "            _loss = tf.reduce_mean(_loss)\n",
    "            train_params = [model.trainable_variables[layer_name_dict[_layer]] for _layer in _log_layers]\n",
    "            gradients=tape.gradient(_loss, train_params)\n",
    "        del tape\n",
    "        \n",
    "        if len(gradients)==1: \n",
    "            grad_dict[loss_names[i]] = gradients[0]\n",
    "        else :\n",
    "            _grads={}\n",
    "            for j in range(len(gradients)):\n",
    "                  _grads[_log_layers[j]]=gradients[j]\n",
    "            grad_dict[loss_names[i]] = _grads\n",
    "\n",
    "    return grad_dict\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  val_loss=0\n",
    "  loss_dict={}\n",
    "  for lbl,pred,loss_name in zip(labels,predictions,loss_funcs_dict.keys()):\n",
    "         loss_func=loss_funcs_dict[loss_name]\n",
    "         loss_weight=loss_weights_dict[loss_name]\n",
    "         _loss,_=loss_func(lbl, pred)\n",
    "         _loss = tf.reduce_mean(_loss)\n",
    "         loss_dict['val_'+loss_name+'_loss']=_loss\n",
    "         val_loss+=loss_weight*_loss\n",
    "  loss_dict['val_loss']=val_loss\n",
    "  return loss_dict,predictions\n",
    "\n",
    "\n",
    "\n",
    "def get_learning_rate(init_lr,epoch):\n",
    "    lr_coeff=_lr_decay_epoch/np.log(_lr_decay_ratio)\n",
    "    _epoch=-_lr_decay_enable*(epoch-1)\n",
    "    epoch_lr=init_lr*np.exp(_epoch/lr_coeff)\n",
    "    if epoch_lr <= _lr_min : epoch_lr= _lr_min\n",
    "    #print('epoch: ',epoch,'lr:', epoch_lr)\n",
    "    return epoch_lr\n",
    "\n",
    "\n",
    "def get_disp_weight(wt_dict,epoch):\n",
    "    init_wt=wt_dict['disp_4']\n",
    "    wt_coeff=_disp_weight_decay_epoch/np.log(_disp_weight_decay_ratio)\n",
    "    _epoch=-_disp_weight_decay_enable*(epoch-1)\n",
    "    epoch_wt=init_wt*np.exp(_epoch/wt_coeff)\n",
    "    if epoch_wt <= _disp_weight_min : epoch_wt= _disp_weight_min\n",
    "    #print('epoch: ',epoch,'lr:', epoch_lr)\n",
    "    wt_dict['disp_4']=np.float32(epoch_wt)\n",
    "    return wt_dict\n",
    "\n",
    "def get_flow_weight(wt_dict,epoch):\n",
    "    init_wt=wt_dict['flow_4']\n",
    "    wt_coeff=_flow_weight_decay_epoch/np.log(_flow_weight_decay_ratio)\n",
    "    _epoch=-_flow_weight_decay_enable*(epoch-1)\n",
    "    epoch_wt=init_wt*np.exp(_epoch/wt_coeff)\n",
    "    if epoch_wt <= _flow_weight_min : epoch_wt= _flow_weight_min\n",
    "    #print('epoch: ',epoch,'lr:', epoch_lr)\n",
    "    wt_dict['flow_4']=np.float32(epoch_wt)\n",
    "    return wt_dict\n",
    "\n",
    "def get_l2_norm(r):\n",
    "    r=tf.square(r)\n",
    "    r=tf.reduce_sum(r)\n",
    "    return r\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    x=x.numpy()\n",
    "    y=y.numpy()\n",
    "    dot_prod=np.sum(x*y)\n",
    "    mag_x=np.sqrt(np.sum(tf.square(x)))\n",
    "    mag_y=np.sqrt(np.sum(tf.square(y)))\n",
    "    _res=dot_prod/(mag_x*mag_y)\n",
    "    #_res=tf.math.acos(_res)\n",
    "    return _res\n",
    "\n",
    "def get_cosine_similarity(_t,_r,_f,_d):\n",
    "    _r=tf.reshape(_r,[-1])\n",
    "    _t=tf.reshape(_t,[-1])\n",
    "    _f=tf.reshape(_f,[-1])\n",
    "    _d=tf.reshape(_d,[-1])\n",
    "\n",
    "    _rf=cosine_similarity(_r,_f)#.numpy()\n",
    "    _tf=cosine_similarity(_t,_f)#.numpy()\n",
    "    _rt=cosine_similarity(_r,_t)#.numpy()\n",
    "    _rd=cosine_similarity(_r,_d)#.numpy()\n",
    "    _td=cosine_similarity(_t,_d)#.numpy()\n",
    "    _fd=cosine_similarity(_f,_d)#.numpy()\n",
    "    _res={}\n",
    "    _res['rf_cos']=_rf\n",
    "    _res['tf_cos']=_tf\n",
    "    _res['rt_cos']=_rt\n",
    "    _res['rd_cos']=_rd\n",
    "    _res['td_cos']=_td\n",
    "    _res['fd_cos']=_fd\n",
    "    return _res\n",
    "\n",
    "class loss_dict_obj(dict):\n",
    "    def sum_update(self,c_dict):\n",
    "        for key,val in c_dict.items():\n",
    "                if key in list(self.keys()):\n",
    "                    self[key]+=tf2np(c_dict[key])\n",
    "                else:\n",
    "                    self[key]=tf2np(c_dict[key])\n",
    "                \n",
    "    \n",
    "    def ext_update(self,c_dict,_ext='_ext'):\n",
    "        for key,val in c_dict.items():\n",
    "                 self[key+_ext]=tf2np(val)\n",
    "                \n",
    "    def divide(self,div_val):\n",
    "        for key,val in self.items():\n",
    "            if type(div_val)==dict or type(div_val)==loss_dict_obj:\n",
    "                self[key]/=div_val[key]\n",
    "            else:\n",
    "                self[key]/=div_val\n",
    "                \n",
    "    def _sum(self):\n",
    "        total=0\n",
    "        for val in self.values():\n",
    "            total+=val\n",
    "        return total\n",
    "    \n",
    "    def copy_keys(self,c_dict,keys):\n",
    "        for key in keys:\n",
    "            self[key]=c_dict[key]\n",
    "            \n",
    "    def apply(self,func):\n",
    "        for key in self.keys():\n",
    "            self[key]=tf2np(func(self[key]))\n",
    "        \n",
    "def tf2np(val):\n",
    "    if hasattr(val,'numpy'):\n",
    "        val=val.numpy()\n",
    "    else:\n",
    "        if val==None: val=0\n",
    "    return val\n",
    "\n",
    "class best_weight_saver():\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.min_val_loss=10e6\n",
    "            self.min_val_mean_loss=10e6\n",
    "            self.min_val_rot_loss=10e6\n",
    "            self.min_val_trans_loss=10e6\n",
    "            if not os.path.exists(log_dir+exp_name+'best_weights') : os.mkdir(log_dir+exp_name+'best_weights')\n",
    "        def save_best_weight(self,_loss_dict,_loss='val_loss',_name='best_model'):\n",
    "            if _loss=='val_loss':_min_loss=self.min_val_loss\n",
    "            if _loss=='val_mean_loss':_min_loss=self.min_val_mean_loss\n",
    "            if _loss=='val_rot_loss':_min_loss=self.min_val_rot_loss\n",
    "            if _loss=='val_trans_loss':_min_loss=self.min_val_trans_loss\n",
    "            if _loss_dict[_loss]<= _min_loss:\n",
    "                _min_loss=_loss_dict[_loss]\n",
    "                model.save(log_dir+exp_name+'best_weights//'+_name+'.h5')\n",
    "                with open(log_dir+exp_name+'best_weights//'+_name+'_info.txt','w') as file:\n",
    "                     file.write('epoch'+':'+str(epoch)+'\\n')\n",
    "                     for key,val in loss_dict.items():\n",
    "                        file.write(str(key)+':'+str(val)+'\\n')\n",
    "            return _min_loss\n",
    "            \n",
    "        def save(self,_loss_dict):\n",
    "            self.min_val_loss=self.save_best_weight(_loss_dict,_loss='val_loss',_name='best_model')\n",
    "            self.min_val_mean_loss=self.save_best_weight(_loss_dict,_loss='val_mean_loss',_name='best_model_mean')\n",
    "            self.min_val_rot_loss=self.save_best_weight(_loss_dict,_loss='val_rot_loss',_name='best_model_rot')\n",
    "            self.min_val_trans_loss=self.save_best_weight(_loss_dict,_loss='val_trans_loss',_name='best_model_trans')\n",
    "            return 0\n",
    "        \n",
    "def get_loss_rate(_loss):\n",
    "    rel_loss_dict=loss_dict_obj()\n",
    "    rel_loss_dict.copy_keys(_loss,['trans_loss','rot_loss','flow_4_loss','disp_4_loss'])\n",
    "    #rel_loss_dict.divide(_init_loss)\n",
    "    #total=rel_loss_dict._sum()\n",
    "    #rel_loss_dict.divide(total)\n",
    "    return rel_loss_dict  \n",
    "        \n",
    "if train == True :\n",
    "    layer_no_dict={}\n",
    "    layer_name_dict={}\n",
    "    #{'grad_rate':_ratio_alpha}\n",
    "    for i,wts in enumerate(model.trainable_variables):\n",
    "            layer_no_dict[i]=wts.name\n",
    "            layer_name_dict[wts.name]=i\n",
    "    \n",
    "    loss_log=loss_logger()\n",
    "    weight_saver=best_weight_saver()\n",
    "    loss_log.on_train_begin()\n",
    "\n",
    "    loss_weights_dict={'trans':_init_loss_wts[0],\n",
    "                       'rot':_init_loss_wts[1],\n",
    "                       'flow_4':_init_loss_wts[2],\n",
    "                       'disp_4':_init_loss_wts[3]}\n",
    "    \n",
    "    cosines=cosine_handler()\n",
    "    slopes=slope_handler()\n",
    "    \n",
    "    _grad_var_dict={'rot':None,'flow_4':None,'disp_4':None}\n",
    "    if debug==True:_total_epochs=start_epoch+25\n",
    "    else:_total_epochs=start_epoch+epochs\n",
    "    for epoch in range(start_epoch,_total_epochs):\n",
    "        print(\"epoch : \",epoch,\"/\",_total_epochs-1)\n",
    "        init_time=time.time()\n",
    "        grads=loss_dict_obj()\n",
    "        loss_dict=loss_dict_obj()\n",
    "        rel_loss_dict=loss_dict_obj()\n",
    "        loss_log.on_epoch_begin(epoch)\n",
    "        \n",
    "        loss_dict['epoch']=epoch\n",
    "        loss_dict['curr_lr']=learning_rate\n",
    "        if _lr_decay_enable == 1:                                               \n",
    "               curr_lr=get_learning_rate(learning_rate,epoch)\n",
    "               curr_optimizer.lr=copy(curr_lr)\n",
    "               loss_dict['curr_lr']=curr_lr\n",
    "                \n",
    "        if _disp_weight_decay_enable==1:\n",
    "               loss_weights_dict=get_disp_weight(loss_weights_dict,epoch)\n",
    "        \n",
    "        if _flow_weight_decay_enable==1:\n",
    "               loss_weights_dict=get_flow_weight(loss_weights_dict,epoch)\n",
    "                \n",
    "        log_raw_grads={\n",
    "            'trans':loss_dict_obj(),\n",
    "            'rot':loss_dict_obj(),\n",
    "            'flow_4':loss_dict_obj(),\n",
    "            'disp_4':loss_dict_obj()\n",
    "            }\n",
    "        \n",
    "       \n",
    "        train_loss=loss_dict_obj()\n",
    "        if debug==True:_tsteps=10\n",
    "        else:_tsteps=len(train_data)//batch_size\n",
    "        curr_grad_var_dict={'rot':None,'flow_4':None,'disp_4':None}\n",
    "        for step in trange(_tsteps):\n",
    "            \n",
    "            images,labels=next(train_batch)\n",
    "            if _mtl_get_variance_from == 'calc' and step%_mtl_grad_var_calc_every==0:\n",
    "                #print('Calculating variance at step : ',step)\n",
    "                curr_grad_var_dict['rot']=get_grad_var(images, labels,'rot')\n",
    "                if _train_flow==True: curr_grad_var_dict['flow_4']=get_grad_var(images, labels,'flow_4')\n",
    "                if _train_disp==True: curr_grad_var_dict['disp_4']=get_grad_var(images, labels,'disp_4')\n",
    "                if _mtl_grad_var_alpha != 1.0:\n",
    "                    for key in curr_grad_var_dict.keys():\n",
    "                        if curr_grad_var_dict[key] ==None: continue\n",
    "                        if curr_grad_var_dict[key] != None and _grad_var_dict[key] != None :\n",
    "                            for _layer_ind in range(len(curr_grad_var_dict[key])):\n",
    "                                _grad_var_dict[key][_layer_ind]=_mtl_grad_var_alpha*curr_grad_var_dict[key][_layer_ind]+(1-_mtl_grad_var_alpha)*_grad_var_dict[key][_layer_ind]\n",
    "                        elif epoch==1 and step==0 :\n",
    "                                _grad_var_dict[key]=curr_grad_var_dict[key]\n",
    "                elif _mtl_grad_var_alpha == 1.0: _grad_var_dict = curr_grad_var_dict \n",
    "                if step%_mtl_update_grad_var==0:\n",
    "                    if _mtl_div_with_rolling_mean==False: _updated_grad_var_dict=_grad_var_dict\n",
    "                    elif _mtl_div_with_rolling_mean==True:\n",
    "                        for key in curr_grad_var_dict.keys():\n",
    "                            if curr_grad_var_dict[key] ==None: continue\n",
    "                            if curr_grad_var_dict[key] != None and _grad_var_dict[key] != None :\n",
    "                                for _layer_ind in range(len(curr_grad_var_dict[key])):\n",
    "                                    curr_grad_var_dict[key][_layer_ind]=curr_grad_var_dict[key][_layer_ind]/(_grad_var_dict[key][_layer_ind]+10e-20)\n",
    "                        _updated_grad_var_dict=curr_grad_var_dict\n",
    "\n",
    "                    \n",
    "            t_loss,calculated_grads,curr_cosines=train_step_mtl(images,labels,_updated_grad_var_dict)\n",
    "            cosines.update(curr_cosines)\n",
    "            curr_rel_loss_dict=get_loss_rate(t_loss)\n",
    "            rel_loss_dict.sum_update(curr_rel_loss_dict)\n",
    "            train_loss.sum_update(t_loss)\n",
    "            \n",
    "        del _updated_grad_var_dict,curr_grad_var_dict\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_loss.divide(len(train_data)//batch_size)\n",
    "        loss_dict.update(train_loss)\n",
    "        \n",
    "        \n",
    "        if epoch==1 : \n",
    "            init_loss=copy(train_loss)\n",
    "        #else:\n",
    "        #    for loss_name in loss_funcs_dict.keys():\n",
    "        #        if train_loss[loss_name+'_loss']>init_loss[loss_name+'_loss']:\n",
    "        #            init_loss[loss_name+'_loss']=train_loss[loss_name+'_loss']\n",
    "        rel_loss_dict.divide((len(train_data)//batch_size))\n",
    "        rel_loss_dict.divide(init_loss)\n",
    "        loss_dict.ext_update(rel_loss_dict,_ext='_rate')    \n",
    "        loss_dict.ext_update(loss_weights_dict,_ext='_weight')\n",
    "        \n",
    "        \n",
    "        slopes.update(_epoch=epoch,_curr_rel_loss=rel_loss_dict)\n",
    "        loss_slope_dict=slopes.log(_epoch=epoch)\n",
    "        loss_dict.update(loss_slope_dict)  \n",
    "        \n",
    "        cosine_log_dict=cosines.log(curr_cosines)\n",
    "        loss_dict.update(cosine_log_dict)\n",
    "        \n",
    "        val_batch   = batch_generator(_val_data,_val_label)\n",
    "        val_preds=np.zeros((1,no_of_pose))\n",
    "        val_gts=np.zeros((1,no_of_pose))\n",
    "        val_loss=loss_dict_obj()\n",
    "        \n",
    "        if debug==True:_vsteps=10\n",
    "        else:_vsteps=len(val_data)//batch_size\n",
    "        for step in trange(_vsteps):\n",
    "            val_images,val_labels=next(val_batch)\n",
    "            v_loss,val_pred=test_step(val_images,val_labels)\n",
    "            val_loss.sum_update(v_loss)\n",
    "            \n",
    "            val_pred=np.concatenate(val_pred[0:2],axis=1)\n",
    "            val_gt=np.concatenate(val_labels[0:2],axis=1)\n",
    "            val_preds=np.concatenate((val_preds,val_pred),axis=0)\n",
    "            val_gts=np.concatenate((val_gts,val_gt),axis=0)\n",
    "            \n",
    "        val_gts=unscl_label(val_gts)\n",
    "        val_preds=unscl_label(val_preds)\n",
    "        if not os.path.exists(log_dir+exp_name+'val_pred'): os.mkdir(log_dir+exp_name+'val_pred')\n",
    "        with open(log_dir+exp_name+'val_pred\\\\'+'val_mat_'+str(epoch)+'.pkl','wb') as file:\n",
    "                    pickle.dump([val_gts,val_preds],file)\n",
    "            \n",
    "            \n",
    "        val_loss.divide(len(val_data)//batch_size)\n",
    "        loss_dict.update(val_loss)        \n",
    "        loss_dict['val_mean_loss']=(loss_dict['val_rot_loss']+loss_dict['val_trans_loss'])/2\n",
    "        weight_saver.save(loss_dict)\n",
    "        \n",
    "        if debug==True:_grad_steps=10\n",
    "        else:_grad_steps=_grad_data_len\n",
    "            \n",
    "        if  epoch%_grad_log_epoch==0 or epoch == 1: \n",
    "           for step in trange(_grad_steps):\n",
    "                images,labels=next(grad_batch)\n",
    "                curr_grads=get_grads(images,labels,grad_log_layers)\n",
    "                for key in list(log_raw_grads.keys()):\n",
    "                     log_raw_grads[key].sum_update(curr_grads[key])\n",
    "           for key in list(log_raw_grads.keys()):\n",
    "                 log_raw_grads[key].divide(_grad_data_len)\n",
    "           for key in list(log_raw_grads.keys()):\n",
    "                 grads[key]=log_raw_grads[key][grad_norm_layers[0]]\n",
    "                  \n",
    "        else :\n",
    "            #for step in trange(_grad_data_len//10):\n",
    "            for step in trange(_grad_steps):\n",
    "                images,labels=next(grad_batch)\n",
    "                curr_grads=get_grads(images,labels,grad_norm_layers)\n",
    "                grads.sum_update(curr_grads)\n",
    "            grads.divide(_grad_data_len)\n",
    "        \n",
    "        cosine_sim=get_cosine_similarity(grads['trans'],grads['rot'],grads['flow_4'],grads['disp_4'])  \n",
    "        loss_dict.update(cosine_sim)\n",
    "        grads.apply(get_l2_norm)\n",
    "        loss_dict.ext_update(grads,_ext='_grad')\n",
    "        loss_log.on_epoch_end(epoch,logs=loss_dict,grads=log_raw_grads,calculated_grads=calculated_grads,_curr_opt=curr_optimizer)\n",
    "        for key,val in loss_dict.items():\n",
    "            print(key,val)\n",
    "        print('time required :', (time.time()-init_time)/60 ,' min')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
