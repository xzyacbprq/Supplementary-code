{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core algorithm related codes anf functions presented for understanding of the algorithm\n",
    "Note that this section of code is just for demonstration. Some part of code like - data preprocessing,network creation, logging results are omited, since they are not necessary for understanding our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function : [train_step_mtl] is the function that updates the weights for one mini-batch\n",
    "        \n",
    "        images->[batch_size x image_w x image_h x 6]\n",
    "        labels [batch_size, trans(3), rot(3), flow[flow_w x flow_hX 2]\n",
    "        --> input : images ,labels\n",
    "        --> Calculate Gradients for main and auxiliary task.\n",
    "        --> Applies clipped gradients to the weights of the model \n",
    "2. Function : [train_step_main] is the training function for main task\n",
    "        \n",
    "        --> input : images, labels [batch_size, trans(3)] or [batch_size, rot(3)]\n",
    "        --> calculates rotation and translation task gradients from corresponding loss\n",
    "        \n",
    "3. Function : [train_step_aux] is the auxiliary task training function (The PRF method)\n",
    "        \n",
    "        --> input : \n",
    "                  -> images, labels [batch_size, flow[flow_w x flow_hX 2]]\n",
    "                  -> Gradient variance for main task and auxiliary task\n",
    "                  -> Mean gradients for main task\n",
    "        --> Calculates optical flow task gradients from corresponding loss.\n",
    "        --> Returns probability ratio x Auxiliary task gradients\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_main(_images, _labels,_task):\n",
    "    _mtp=train_params_tracker(_task)\n",
    "    _loss_funcs=list(loss_funcs_dict.keys())\n",
    "    _n=_loss_funcs.index(_task)\n",
    "    \n",
    "    #Calculate gradients for main task\n",
    "    with tf.GradientTape() as tape:\n",
    "        _predictions = model(_images, training=True)\n",
    "        lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "\n",
    "        loss_func=loss_funcs_dict[loss_name]\n",
    "        loss_weight=loss_weights_dict[loss_name]\n",
    "\n",
    "        _batch_loss=loss_func(lbl, pred)\n",
    "        _loss = tf.reduce_mean(_batch_loss)\n",
    "        _gradients = tape.gradient(_loss, model.trainable_variables) \n",
    "        \n",
    "        #Log necessary variables\n",
    "        _mtp._loss_dict[loss_name+'_loss']=_loss\n",
    "        _mtp._loss_dict['train_loss']=loss_weight*_loss\n",
    "        _mtp._grad_dict[loss_name+'_grad'] = _gradients\n",
    "        \n",
    "    del tape\n",
    "    for i in range(len(_mtp.total_gradients)):\n",
    "        if _gradients[i]!=None: _mtp.total_gradients[i]= tf.multiply(loss_weight,_gradients[i])\n",
    "\n",
    "    return [_mtp.__dict__]\n",
    "\n",
    "@tf.function\n",
    "def train_step_aux(_images, _labels,_task,_mtp,_grad_var_dict):\n",
    "        #Extract main task and auxiliary task variances\n",
    "        #Extract main task gradients\n",
    "        _atp=train_params_tracker(_task)\n",
    "        _loss_funcs=list(loss_funcs_dict.keys())\n",
    "        _main_task=_loss_funcs[_mtl_main_task_index]\n",
    "        _n=_loss_funcs.index(_task)\n",
    "        _main_task_gradients=_mtp['_grad_dict'][_main_task+'_grad'] #Extract main task gradients\n",
    "        _main_task_grad_var=_grad_var_dict[_main_task] #Extract main task variances\n",
    "        _aux_task_grad_var=_grad_var_dict[_task] #Extract auxiliary task variances\n",
    "        \n",
    "        #Calculate auxiliary task gradients\n",
    "        with tf.GradientTape() as tape: \n",
    "            _predictions = model(_images, training=True)\n",
    "\n",
    "            #Get loss function and loss weight beta\n",
    "            lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "            loss_func=loss_funcs_dict[loss_name]\n",
    "            loss_weight=loss_weights_dict[loss_name]\n",
    "\n",
    "            _batch_loss,_abs_loss_b=loss_func(lbl, pred)\n",
    "            _loss_b = tf.reduce_mean(_batch_loss)\n",
    "            _aux_task_gradients=tape.gradient(_loss_b, model.trainable_variables) \n",
    "            \n",
    "            #Log loss and gradients\n",
    "            _atp._loss_dict[loss_name+'_loss']=_loss_b\n",
    "            _atp._loss_dict['train_loss']=loss_weight*_loss_b\n",
    "            _atp._grad_dict[loss_name+'_grad'] = _aux_task_gradients\n",
    "\n",
    "        del tape\n",
    "        \n",
    "        _beta_aux=loss_weight\n",
    "        for k in range(len(_atp.total_gradients)):\n",
    "                if _main_task_gradients[k] != None and _aux_task_gradients[k] != None: # Multiply shared weight gradients by factors\n",
    "                        # v1 is for vanila auxiliary task guidance\n",
    "                        if exp == 'ATG': \n",
    "                                _atp.total_gradients[k]=tf.multiply(_beta_aux,_aux_task_gradients[k])\n",
    "                                \n",
    "                        #v-4 is for PRF\n",
    "                        _min_std = 10e-20\n",
    "                        if exp == 'PRF':\n",
    "                            _var_main_task=_main_grad_var[k]+_min_std #_min_std is added to avoid division by zero\n",
    "                            _var_main_task=_aux_grad_var[k]+_min_std\n",
    "\n",
    "                            _conf_ratio=_var_aux_task/_var_main_task  \n",
    "                            _conf_ratio=tf.sqrt(_conf_ratio)\n",
    "                            _task_similarity=tf.square((_main_task_gradients[k]-_aux_task_gradients[k]))\n",
    "                            _task_similarity=_task_similarity/_var_main_task\n",
    "                            _task_similarity=tf.exp(-0.5*_task_similarity)\n",
    "                            _prob_ratio=tf.multiply(_conf_ratio,_task_similarity) \n",
    "                            _atp.total_gradients[k]=tf.multiply((_beta_aux*_prob_ratio),_aux_task_gradients[k])\n",
    "                                                                        \n",
    "                elif _aux_task_gradients[k] != None: # Update task specific gradients  \n",
    "                     _atp.total_gradients[k]= _beta_aux*_aux_task_gradients[k]\n",
    "        \n",
    "        return [_atp.__dict__]\n",
    "    \n",
    "#@tf.function\n",
    "def train_step_mtl(_images,_labels,_grad_var_dict={}):\n",
    "    _updated_train_params=train_params_tracker()\n",
    "    _train_params=[]      \n",
    "    _train_params+=train_step_main(_images,_labels,'trans')\n",
    "    _train_params+=train_step_main(_images,_labels,'rot')\n",
    "    _train_params+=train_step_aux(_images, _labels,'flow_4',_train_params[_mtl_main_task_index],_grad_var_dict)\n",
    "    _updated_train_params.update_all(_train_params)\n",
    "    \n",
    "    del _train_params\n",
    "    apply_clipped_grads(_updated_train_params.total_gradients)\n",
    "    _grad_dict = _updated_train_params._grad_dict\n",
    "    \n",
    "    _loss_dict = _updated_train_params._loss_dict\n",
    "    del _updated_train_params\n",
    "\n",
    "    return _loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "1. Function : [get_grad_var] function for calculating variance of gradients. (Gradient variance is calculated here for PRF)\n",
    "        \n",
    "        --> takes input samples and labels\n",
    "        --> Calculates batch of gradients and corresponding variances\n",
    "\n",
    "2. Function : [apply_clipped_grads]  for applying clipped gradients\n",
    "\n",
    "3. Function : [train_params_tracker] a utility function for keeping track of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function        \n",
    "def get_grad_var(_images, _labels,_task):\n",
    "        _loss_funcs=list(loss_funcs_dict.keys())\n",
    "        _n=_loss_funcs.index(_task)\n",
    "        \n",
    "        _grad_variance=[None]*36 #36 is the total number of shared layers(including BatchNorm and Leaky ReLU)\n",
    "        _sample_grads_list=[None]*batch_size\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape: \n",
    "            _predictions = model(_images, training=True)\n",
    "\n",
    "            lbl,pred,loss_name=_labels[_n],_predictions[_n],_loss_funcs[_n]\n",
    "            loss_func=loss_funcs_dict[loss_name]\n",
    "\n",
    "            _batch_loss=loss_func(lbl, pred)\n",
    "            _loss_b = tf.reduce_mean(_batch_loss)\n",
    "\n",
    "            _mean_grads=tape.gradient(_loss_b, model.trainable_variables[:36])\n",
    "            \n",
    "            for k in range(batch_size):\n",
    "                _sample_grads=tape.gradient(_batch_loss[k], model.trainable_variables[:36]) \n",
    "                _sample_grads_list[k]=_sample_grads\n",
    "                \n",
    "        del tape                      \n",
    "        for _sample_grads in _sample_grads_list:\n",
    "            for k,(_g1,_g2) in enumerate(zip(_mean_grads,_sample_grads)):\n",
    "                if _grad_variance[k] != None : _grad_variance[k]=_grad_variance[k]+(1/batch_size)*tf.square(_g1-_g2)\n",
    "                else : _grad_variance[k]= (1/batch_size)*tf.square(_g1-_g2)\n",
    "                    \n",
    "        return _grad_variance\n",
    "            \n",
    "\n",
    "\n",
    "def apply_clipped_grads(total_gradients):\n",
    "    if _grad_clip != 0:\n",
    "        for  _layer in grad_clip_layers.keys():\n",
    "            _clip_value=grad_clip_layers[_layer]\n",
    "            ind=layer_name_dict[_layer]\n",
    "            total_gradients[ind]= tf.clip_by_value(total_gradients[ind],-_clip_value,_clip_value) \n",
    "\n",
    "    curr_optimizer.apply_gradients(zip(total_gradients, model.trainable_variables))\n",
    "    return 1\n",
    "\n",
    "class train_params_tracker():\n",
    "    def __init__(self,_task=None):\n",
    "        self.total_gradients=[None]*len(model.trainable_variables)\n",
    "        self._grad_dict={}\n",
    "        self._std_dict={}\n",
    "        self._loss_dict={'train_loss':0.0,'rot_loss':0.0,'trans_loss':0.0,'flow_4_loss':0.0,'disp_4_loss':0.0}\n",
    "        \n",
    "    def update(self,obj):\n",
    "        for i,(gr1,gr2) in enumerate(zip(self.total_gradients,obj['total_gradients'])):\n",
    "            if gr1 ==None: gr1=0.0\n",
    "            if gr2 ==None: gr2=0.0\n",
    "            self.total_gradients[i]=gr1+gr2\n",
    "        for key in self._loss_dict.keys():\n",
    "            self._loss_dict[key]+=obj['_loss_dict'][key]\n",
    "        self._grad_dict.update(obj['_grad_dict'])\n",
    "        self._std_dict.update(obj['_std_dict'])\n",
    "    def update_all(self,list_obj):\n",
    "        for obj in list_obj:\n",
    "            self.update(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_loss(y_true, y_pred):\n",
    "        y_true=tf.cast(y_true,tf.float32)\n",
    "\n",
    "        y_true=tf.clip_by_value(tf.cast(y_true,tf.float32),0.0,1.0)\n",
    "        y_pred=tf.clip_by_value(tf.cast(y_pred,tf.float32),0.0,1.0)\n",
    "        _add_offset=1\n",
    "\n",
    "        y_true=tf.math.log(y_true+_add_offset)\n",
    "        y_pred=tf.math.log(y_pred+_add_offset)\n",
    "\n",
    "        loss=tf.square(tf.subtract(y_true,y_pred))\n",
    "        abs_loss=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=[1,2,3])\n",
    "        loss=tf.sqrt(tf.reduce_mean(loss,axis=[1,2,3]))\n",
    "        return loss,abs_loss \n",
    "\n",
    "\n",
    "def trans_loss(y_true,y_pred):\n",
    "    abs_loss_trans=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    loss_trans=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    return loss_trans,abs_loss_trans\n",
    "\n",
    "def rot_loss(y_true,y_pred):\n",
    "    abs_loss_rot=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    loss_rot=tf.reduce_mean(tf.abs(tf.subtract(y_true,y_pred)),axis=-1)\n",
    "    return loss_rot,abs_loss_rot\n",
    "\n",
    "_mtl_main_task_index =1 #rotation\n",
    "loss_funcs_dict={'trans':trans_loss_func,\n",
    "                 'rot':rot_loss_func,\n",
    "                 'flow_4':flow_loss_func\n",
    "                }\n",
    "loss_weights_dict={\n",
    "                'trans':1.0\n",
    "                'rot':10.0\n",
    "                'flow':0.1\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (bare minimum code is presented here for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    \n",
    "    for step in range(no_of_train_batches):\n",
    "        _images,_labels=get_next_batch(train_data)\n",
    "        _grad_var_dict['rot']=get_grad_var(_images, _labels,'rot') #main task\n",
    "        _grad_var_dict['flow_4']=get_grad_var(_images, _labels,'flow_4')#auxiliary task\n",
    "        train_loss=train_step_mtl(_images,_labels,_grad_var_dict=_grad_var_dict)\n",
    "        \n",
    "    for step in range(no_of_val_batches):\n",
    "        _images,_labels=get_next_batch(val_data)\n",
    "        val_loss=test_step(_images,_labels)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
